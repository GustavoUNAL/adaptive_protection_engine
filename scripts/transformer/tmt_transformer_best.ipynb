{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando leer el archivo: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json\n",
      "Archivo leído exitosamente.\n",
      "Modificando los valores de 'Time_out' a null...\n",
      "Modificación completada en memoria.\n",
      "Intentando guardar el archivo actualizado: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json\n",
      "¡Éxito! El archivo /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json ha sido actualizado.\n",
      "Todos los valores asociados a la clave 'Time_out' han sido cambiados a null.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os # Import the os module for path manipulation if needed\n",
    "\n",
    "# Define the path to the JSON file\n",
    "# IMPORTANT: Make sure this path is correct for your system.\n",
    "# Using os.path.expanduser('~') can help make paths more portable if the file\n",
    "# is relative to the user's home directory, but for an absolute path like this,\n",
    "# just ensure it's correct.\n",
    "file_path = '/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json'\n",
    "\n",
    "# --- Recursive function to find and update \"Time_out\" values ---\n",
    "# This function handles nested dictionaries and lists within the JSON structure.\n",
    "def update_time_out_values(data_item):\n",
    "    \"\"\"\n",
    "    Recursively traverses a data structure (dictionary or list)\n",
    "    and sets the value associated with the key \"Time_out\" to None.\n",
    "    \"\"\"\n",
    "    if isinstance(data_item, dict):\n",
    "        for key, value in data_item.items():\n",
    "            if key == \"Time_out\":\n",
    "                # Found the key, set its value to None (which becomes null in JSON)\n",
    "                data_item[key] = None\n",
    "            else:\n",
    "                # If the value is another dict or a list, recurse into it\n",
    "                update_time_out_values(value)\n",
    "    elif isinstance(data_item, list):\n",
    "        # If it's a list, iterate through its items and recurse\n",
    "        for item in data_item:\n",
    "            update_time_out_values(item)\n",
    "    # Base case: If it's not a dict or list (e.g., str, int, float, bool, None), do nothing.\n",
    "\n",
    "# --- Main execution block ---\n",
    "try:\n",
    "    # Step 1: Read the JSON file\n",
    "    print(f\"Intentando leer el archivo: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Load the JSON data from the file into a Python object (list or dict)\n",
    "        data = json.load(f)\n",
    "    print(\"Archivo leído exitosamente.\")\n",
    "\n",
    "    # Step 2: Modify the data\n",
    "    print(\"Modificando los valores de 'Time_out' a null...\")\n",
    "    # Call the recursive function to update \"Time_out\" values throughout the data\n",
    "    update_time_out_values(data)\n",
    "    print(\"Modificación completada en memoria.\")\n",
    "\n",
    "    # Step 3: Write the updated data back to the same file\n",
    "    print(f\"Intentando guardar el archivo actualizado: {file_path}\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        # Dump the modified Python object back into the JSON file\n",
    "        # ensure_ascii=False is good practice for handling non-ASCII characters\n",
    "        # indent=4 makes the output JSON file human-readable (optional but recommended)\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"¡Éxito! El archivo {file_path} ha sido actualizado.\")\n",
    "    print(\"Todos los valores asociados a la clave 'Time_out' han sido cambiados a null.\")\n",
    "\n",
    "# --- Error Handling ---\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se pudo encontrar el archivo en la ruta especificada.\")\n",
    "    print(f\"Verifica que la ruta '{file_path}' sea correcta.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: El archivo '{file_path}' no contiene JSON válido o está corrupto.\")\n",
    "    print(\"Verifica el contenido del archivo.\")\n",
    "except PermissionError:\n",
    "    print(f\"Error: No tienes permisos para leer o escribir en el archivo '{file_path}'.\")\n",
    "    print(\"Verifica los permisos del archivo o la carpeta.\")\n",
    "except Exception as e:\n",
    "    # Catch any other unexpected errors\n",
    "    print(f\"Ocurrió un error inesperado:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo de predicciones: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/results/predictions_best.json\n",
      "Archivo de predicciones leído.\n",
      "Datos de predicciones organizados para búsqueda.\n",
      "Leyendo archivo de datos de entrada: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json\n",
      "Archivo de datos de entrada leído.\n",
      "Iniciando la combinación de datos...\n",
      "Combinación completada. Entradas actualizadas: 3400, Entradas omitidas/sin coincidencia: 0\n",
      "Guardando los datos actualizados en: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json\n",
      "¡Éxito! El archivo /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json ha sido actualizado con los valores de 'pick_up' y 'TDS'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the paths to the JSON files\n",
    "predictions_path = '/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/results/predictions_best.json'\n",
    "input_data_path = '/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json'\n",
    "# Output path will be the same as the input data path, overwriting it\n",
    "output_path = input_data_path\n",
    "\n",
    "# --- Helper function to safely update dictionary ---\n",
    "def safe_update_relay_data(target_relay, source_relay):\n",
    "    \"\"\"\n",
    "    Safely updates the target relay dict with 'pick_up' and 'TDS'\n",
    "    from the source relay dict, if they exist.\n",
    "    \"\"\"\n",
    "    if source_relay:\n",
    "        if 'pick_up' in source_relay:\n",
    "            target_relay['pick_up'] = source_relay['pick_up']\n",
    "        if 'TDS' in source_relay:\n",
    "            target_relay['TDS'] = source_relay['TDS']\n",
    "\n",
    "# --- Main execution block ---\n",
    "try:\n",
    "    # Step 1: Read the predictions data\n",
    "    print(f\"Leyendo archivo de predicciones: {predictions_path}\")\n",
    "    if not os.path.exists(predictions_path):\n",
    "        raise FileNotFoundError(f\"El archivo de predicciones no existe: {predictions_path}\")\n",
    "    with open(predictions_path, 'r', encoding='utf-8') as f:\n",
    "        predictions_list = json.load(f)\n",
    "    print(\"Archivo de predicciones leído.\")\n",
    "\n",
    "    # Step 2: Organize predictions for efficient lookup\n",
    "    # Create a dictionary where keys are scenario_id and values are dictionaries\n",
    "    # mapping pair_index to the corresponding prediction data.\n",
    "    predictions_map = defaultdict(dict)\n",
    "    for prediction in predictions_list:\n",
    "        scenario_id = prediction.get(\"scenario_id\")\n",
    "        pair_index = prediction.get(\"pair_index\")\n",
    "        if scenario_id is not None and pair_index is not None:\n",
    "            predictions_map[scenario_id][pair_index] = prediction\n",
    "        else:\n",
    "            print(f\"Advertencia: Entrada de predicción omitida por falta de 'scenario_id' o 'pair_index': {prediction}\")\n",
    "    print(\"Datos de predicciones organizados para búsqueda.\")\n",
    "\n",
    "    # Step 3: Read the input data file\n",
    "    print(f\"Leyendo archivo de datos de entrada: {input_data_path}\")\n",
    "    if not os.path.exists(input_data_path):\n",
    "        raise FileNotFoundError(f\"El archivo de datos de entrada no existe: {input_data_path}\")\n",
    "    with open(input_data_path, 'r', encoding='utf-8') as f:\n",
    "        input_data_list = json.load(f)\n",
    "    print(\"Archivo de datos de entrada leído.\")\n",
    "\n",
    "    # Step 4: Merge the data\n",
    "    print(\"Iniciando la combinación de datos...\")\n",
    "    # Keep track of the pair index for each scenario as we iterate through input_data\n",
    "    scenario_pair_counters = defaultdict(int)\n",
    "    skipped_entries = 0\n",
    "    updated_entries = 0\n",
    "\n",
    "    for input_entry in input_data_list:\n",
    "        scenario_id = input_entry.get(\"scenario_id\")\n",
    "\n",
    "        if not scenario_id:\n",
    "            print(f\"Advertencia: Entrada de datos omitida por falta de 'scenario_id': {input_entry}\")\n",
    "            skipped_entries += 1\n",
    "            continue\n",
    "\n",
    "        # Determine the current pair index for this scenario based on the order in the input file\n",
    "        current_pair_index = scenario_pair_counters[scenario_id]\n",
    "        scenario_pair_counters[scenario_id] += 1 # Increment for the next entry of the same scenario\n",
    "\n",
    "        # Find the corresponding prediction data\n",
    "        prediction_data = predictions_map.get(scenario_id, {}).get(current_pair_index)\n",
    "\n",
    "        if prediction_data:\n",
    "            # Update main relay data if it exists in both entries\n",
    "            if 'main_relay' in input_entry:\n",
    "                safe_update_relay_data(input_entry['main_relay'], prediction_data.get('main_relay'))\n",
    "\n",
    "            # Update backup relay data if it exists in both entries\n",
    "            if 'backup_relay' in input_entry:\n",
    "                 safe_update_relay_data(input_entry['backup_relay'], prediction_data.get('backup_relay'))\n",
    "\n",
    "            updated_entries += 1\n",
    "        else:\n",
    "            # Optional: Add a warning if no matching prediction was found for an input entry\n",
    "            # This might happen if the number of pairs per scenario differs between files\n",
    "            # or if scenario_ids don't match.\n",
    "            print(f\"Advertencia: No se encontraron datos de predicción para scenario_id='{scenario_id}', pair_index={current_pair_index} (índice basado en orden).\")\n",
    "            skipped_entries += 1\n",
    "\n",
    "    print(f\"Combinación completada. Entradas actualizadas: {updated_entries}, Entradas omitidas/sin coincidencia: {skipped_entries}\")\n",
    "\n",
    "    # Step 5: Write the updated data back to the input file (overwrite)\n",
    "    print(f\"Guardando los datos actualizados en: {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        # Use indent=4 for readability\n",
    "        json.dump(input_data_list, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"¡Éxito! El archivo {output_path} ha sido actualizado con los valores de 'pick_up' y 'TDS'.\")\n",
    "\n",
    "# --- Error Handling ---\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Archivo no encontrado.\")\n",
    "    print(e)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: Falla al decodificar JSON. Verifica que ambos archivos sean JSON válidos.\")\n",
    "    print(f\"Detalles del error: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Falta una clave esperada en los datos.\")\n",
    "    print(f\"Clave faltante: {e}\")\n",
    "    print(\"Verifica la estructura de tus archivos JSON.\")\n",
    "except Exception as e:\n",
    "    # Catch any other unexpected errors\n",
    "    print(f\"Ocurrió un error inesperado:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculo del TMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo de datos: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json\n",
      "Archivo de datos leído.\n",
      "Iniciando cálculos (K=0.14, N=0.02, CTI=0.25)...\n",
      "Cálculos completados para 3400 entradas totales.\n",
      "\n",
      "--- Suma de Miscoordination Time (TMT) por Escenario ---\n",
      "  - Escenario 'scenario_1': tmt_total = -5.4128\n",
      "  - Escenario 'scenario_21': tmt_total = -3.9322\n",
      "  - Escenario 'scenario_22': tmt_total = -3.9573\n",
      "  - Escenario 'scenario_23': tmt_total = -4.2042\n",
      "  - Escenario 'scenario_24': tmt_total = -5.1529\n",
      "  - Escenario 'scenario_25': tmt_total = -4.6006\n",
      "  - Escenario 'scenario_26': tmt_total = -3.3028\n",
      "  - Escenario 'scenario_27': tmt_total = -6.0539\n",
      "  - Escenario 'scenario_28': tmt_total = -5.5898\n",
      "  - Escenario 'scenario_29': tmt_total = -5.4373\n",
      "  - Escenario 'scenario_30': tmt_total = -3.4917\n",
      "  - Escenario 'scenario_31': tmt_total = -3.4995\n",
      "  - Escenario 'scenario_32': tmt_total = -3.7007\n",
      "  - Escenario 'scenario_33': tmt_total = -5.4504\n",
      "  - Escenario 'scenario_34': tmt_total = -5.0606\n",
      "  - Escenario 'scenario_35': tmt_total = -6.1126\n",
      "  - Escenario 'scenario_36': tmt_total = -3.7068\n",
      "  - Escenario 'scenario_4': tmt_total = -5.7451\n",
      "  - Escenario 'scenario_53': tmt_total = -3.7852\n",
      "  - Escenario 'scenario_54': tmt_total = -4.6097\n",
      "  - Escenario 'scenario_55': tmt_total = -4.1879\n",
      "  - Escenario 'scenario_56': tmt_total = -4.5796\n",
      "  - Escenario 'scenario_57': tmt_total = -5.0193\n",
      "  - Escenario 'scenario_58': tmt_total = -4.5685\n",
      "  - Escenario 'scenario_59': tmt_total = -4.4978\n",
      "  - Escenario 'scenario_60': tmt_total = -3.8419\n",
      "  - Escenario 'scenario_61': tmt_total = -4.1820\n",
      "  - Escenario 'scenario_62': tmt_total = -4.3186\n",
      "  - Escenario 'scenario_63': tmt_total = -5.0629\n",
      "  - Escenario 'scenario_64': tmt_total = -3.4653\n",
      "  - Escenario 'scenario_65': tmt_total = -4.5940\n",
      "  - Escenario 'scenario_66': tmt_total = -3.5736\n",
      "  - Escenario 'scenario_67': tmt_total = -3.9439\n",
      "  - Escenario 'scenario_68': tmt_total = -3.4870\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Resumen General de Coordinación (Todos los Escenarios) ---\n",
      "Total de Pares Procesados: 3400\n",
      "  - Pares Coordinados (delta_t >= 0): 2072\n",
      "  - Pares Descoordinados (delta_t < 0): 1328\n",
      "  - Pares Inválidos/Sin Operación (tiempo infinito o datos faltantes): 0\n",
      "----------------------------------------------------------\n",
      "\n",
      "Guardando los datos actualizados en: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json\n",
      "¡Éxito! El archivo /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json ha sido actualizado con los datos por par.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict # Import defaultdict for easier summing per scenario\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the JSON file (updated in previous steps)\n",
    "input_data_path = '/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/input_data_best.json'\n",
    "# Output path will be the same, overwriting the file\n",
    "output_path = input_data_path\n",
    "\n",
    "# --- Constants for Calculation ---\n",
    "# IEC Standard Normal Inverse Curve parameters\n",
    "K = 0.14\n",
    "N = 0.02\n",
    "# Coordination Time Interval (CTI) in seconds\n",
    "CTI = 0.25\n",
    "\n",
    "# --- Function to calculate operation time ---\n",
    "# (Function remains the same as previous version)\n",
    "def calculate_operation_time(I_shc, I_pi, TDS, relay_id=\"N/A\"):\n",
    "    if I_shc is None or I_pi is None or TDS is None: return float('inf')\n",
    "    if not all(isinstance(x, (int, float)) for x in [I_shc, I_pi, TDS]): return float('inf')\n",
    "    if I_pi <= 0: return float('inf')\n",
    "    if TDS <= 0: return float('inf')\n",
    "    try:\n",
    "        M = I_shc / I_pi\n",
    "        if M <= 1: return float('inf')\n",
    "        denominator = (M**N) - 1\n",
    "        if denominator <= 1e-9: return float('inf')\n",
    "        time = (K / denominator) * TDS\n",
    "        return max(0.0, time)\n",
    "    except (ZeroDivisionError, OverflowError, ValueError):\n",
    "        return float('inf')\n",
    "\n",
    "# --- Main execution block ---\n",
    "try:\n",
    "    # Step 1: Read the input data file\n",
    "    print(f\"Leyendo archivo de datos: {input_data_path}\")\n",
    "    if not os.path.exists(input_data_path):\n",
    "        raise FileNotFoundError(f\"El archivo de datos de entrada no existe: {input_data_path}\")\n",
    "    with open(input_data_path, 'r', encoding='utf-8') as f:\n",
    "        data_list = json.load(f)\n",
    "    print(\"Archivo de datos leído.\")\n",
    "\n",
    "    # Step 2: Iterate, calculate, handle values, and sum MT per scenario\n",
    "    print(f\"Iniciando cálculos (K={K}, N={N}, CTI={CTI})...\")\n",
    "\n",
    "    # Counters for overall summary\n",
    "    processed_pairs = 0\n",
    "    coordinated_count = 0\n",
    "    miscoordinated_count = 0\n",
    "    invalid_or_no_op_count = 0\n",
    "\n",
    "    # Dictionary to store TMT sum per scenario\n",
    "    # defaultdict(float) initializes new keys with 0.0\n",
    "    scenario_tmt_totals = defaultdict(float)\n",
    "\n",
    "    for entry in data_list:\n",
    "        processed_pairs += 1\n",
    "        main_relay_data = entry.get('main_relay')\n",
    "        backup_relay_data = entry.get('backup_relay')\n",
    "        # Get scenario ID early, use a default if missing\n",
    "        scenario_id = entry.get('scenario_id', 'ID_Desconocido')\n",
    "\n",
    "        if not main_relay_data or not backup_relay_data:\n",
    "            print(f\"Advertencia: Entrada {processed_pairs} (Escenario: {scenario_id}) omitida por falta de 'main_relay' o 'backup_relay'.\")\n",
    "            if main_relay_data: main_relay_data['Time_out'] = None\n",
    "            if backup_relay_data: backup_relay_data['Time_out'] = None\n",
    "            entry['delta_t'] = None\n",
    "            entry['MT'] = None\n",
    "            invalid_or_no_op_count += 1\n",
    "            continue # Skip to next entry\n",
    "\n",
    "        main_relay_id = main_relay_data.get('relay', 'Principal_Desconocido')\n",
    "        backup_relay_id = backup_relay_data.get('relay', 'Respaldo_Desconocido')\n",
    "\n",
    "        # --- Get values and Force Positive Pickup/TDS ---\n",
    "        I_shc_main = main_relay_data.get('Ishc')\n",
    "        I_pi_main = main_relay_data.get('pick_up')\n",
    "        TDS_main = main_relay_data.get('TDS')\n",
    "        I_shc_backup = backup_relay_data.get('Ishc')\n",
    "        I_pi_backup = backup_relay_data.get('pick_up')\n",
    "        TDS_backup = backup_relay_data.get('TDS')\n",
    "\n",
    "        if I_pi_main is not None and isinstance(I_pi_main, (int, float)): I_pi_main = abs(I_pi_main)\n",
    "        if TDS_main is not None and isinstance(TDS_main, (int, float)): TDS_main = abs(TDS_main)\n",
    "        if I_pi_backup is not None and isinstance(I_pi_backup, (int, float)): I_pi_backup = abs(I_pi_backup)\n",
    "        if TDS_backup is not None and isinstance(TDS_backup, (int, float)): TDS_backup = abs(TDS_backup)\n",
    "        # --- End forcing positive ---\n",
    "\n",
    "        t_m_ref = calculate_operation_time(I_shc_main, I_pi_main, TDS_main, main_relay_id)\n",
    "        t_b_ref = calculate_operation_time(I_shc_backup, I_pi_backup, TDS_backup, backup_relay_id) # Using Backup Ishc\n",
    "\n",
    "        main_relay_data['Time_out'] = t_m_ref if t_m_ref != float('inf') else None\n",
    "        backup_relay_data['Time_out'] = t_b_ref if t_b_ref != float('inf') else None\n",
    "\n",
    "        delta_t = None\n",
    "        mt = None\n",
    "\n",
    "        if t_m_ref != float('inf') and t_b_ref != float('inf'):\n",
    "            delta_t = t_b_ref - t_m_ref - CTI\n",
    "            mt = min(0.0, (delta_t - abs(delta_t)) / 2)\n",
    "\n",
    "            # --- Accumulate MT per scenario ---\n",
    "            # Add MT to the total for this specific scenario *only if* it's negative\n",
    "            if mt < 0:\n",
    "                 scenario_tmt_totals[scenario_id] += mt\n",
    "            # --- End Accumulate ---\n",
    "\n",
    "            if delta_t >= 0:\n",
    "                coordinated_count += 1\n",
    "            else:\n",
    "                miscoordinated_count += 1\n",
    "        else:\n",
    "            invalid_or_no_op_count += 1\n",
    "\n",
    "        entry['delta_t'] = delta_t\n",
    "        entry['MT'] = mt\n",
    "        # End of loop for one entry\n",
    "\n",
    "    print(f\"Cálculos completados para {processed_pairs} entradas totales.\")\n",
    "\n",
    "    # Step 3: Print Summary per Scenario\n",
    "    print(\"\\n--- Suma de Miscoordination Time (TMT) por Escenario ---\")\n",
    "    if scenario_tmt_totals:\n",
    "        # Sort scenarios by ID for consistent output order\n",
    "        sorted_scenarios = sorted(scenario_tmt_totals.items())\n",
    "        for scenario_id, tmt in sorted_scenarios:\n",
    "            print(f\"  - Escenario '{scenario_id}': tmt_total = {tmt:.4f}\")\n",
    "    else:\n",
    "        print(\"  No se registraron valores de TMT negativos en ningún escenario.\")\n",
    "    print(\"----------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "    # Step 4: Print Overall Summary\n",
    "    print(\"--- Resumen General de Coordinación (Todos los Escenarios) ---\")\n",
    "    print(f\"Total de Pares Procesados: {processed_pairs}\")\n",
    "    print(f\"  - Pares Coordinados (delta_t >= 0): {coordinated_count}\")\n",
    "    print(f\"  - Pares Descoordinados (delta_t < 0): {miscoordinated_count}\")\n",
    "    print(f\"  - Pares Inválidos/Sin Operación (tiempo infinito o datos faltantes): {invalid_or_no_op_count}\")\n",
    "    print(f\"----------------------------------------------------------\")\n",
    "    # Verification check\n",
    "    if processed_pairs != (coordinated_count + miscoordinated_count + invalid_or_no_op_count):\n",
    "         print(\"ADVERTENCIA: El conteo de pares no coincide con el total procesado.\")\n",
    "    print(\"\") # Add a newline for spacing\n",
    "\n",
    "    # Step 5: Write the updated data (with individual Time_out, delta_t, MT per pair) back to the file\n",
    "    print(f\"Guardando los datos actualizados en: {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        # The main JSON file still contains the list of pairs, each updated.\n",
    "        # The per-scenario TMT totals are only printed, not added back to this file.\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"¡Éxito! El archivo {output_path} ha sido actualizado con los datos por par.\")\n",
    "\n",
    "# --- Error Handling ---\n",
    "except FileNotFoundError as e: print(f\"Error: Archivo no encontrado.\\n{e}\")\n",
    "except json.JSONDecodeError as e: print(f\"Error: Falla al decodificar JSON en {input_data_path}.\\nDetalles: {e}\")\n",
    "except KeyError as e: print(f\"Error: Falta una clave esperada en los datos JSON.\\nClave faltante: {e}\")\n",
    "except TypeError as e: print(f\"Error de tipo: Probablemente un valor None o tipo incorrecto.\\nDetalles: {e}\")\n",
    "except Exception as e: print(f\"Ocurrió un error inesperado:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Genera:\n",
    "  • summary_scenarios.csv  –  métrica global por escenario\n",
    "  • mt_distribution.csv    –  |mt| de cada par descoordinado\n",
    "Notas:\n",
    "  – TMT se define como la suma de mt (siempre negativo) y es adimensional.\n",
    "  – CTI fijo en 0.2 s.\n",
    "\"\"\"\n",
    "import json, csv, os\n",
    "CTI = 0.2\n",
    "INPUT_FILE = \"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/independent_relay_pairs_best.json\"\n",
    "OUT_CSV_SCEN = \"summary_scenarios_transformer.csv\"\n",
    "OUT_CSV_MT   = \"mt_distribution_transformer.csv\"\n",
    "\n",
    "with open(INPUT_FILE) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "per_scenario = {}\n",
    "for p in data:\n",
    "    sid = p[\"scenario_id\"]\n",
    "    d = per_scenario.setdefault(sid, {\"total\":0,\"valid\":0,\"descoord\":0,\"TMT\":0.0})\n",
    "    d[\"total\"] += 1\n",
    "    tm, tb = p[\"main_relay\"].get(\"Time_out\"), p[\"backup_relay\"].get(\"Time_out\")\n",
    "    if not all(isinstance(x,(int,float)) for x in (tm, tb)):\n",
    "        continue\n",
    "    d[\"valid\"] += 1\n",
    "    delta_t = tb - tm - CTI\n",
    "    mt = (delta_t - abs(delta_t))/2        # negativo si Δt<0\n",
    "    if delta_t < 0:\n",
    "        d[\"descoord\"] += 1\n",
    "        d[\"TMT\"]      += mt                # suma valores negativos\n",
    "        # exportar |mt| para gráfica de distribución\n",
    "        with open(OUT_CSV_MT,\"a\",newline=\"\") as g:\n",
    "            csv.writer(g).writerow([\n",
    "                sid,\n",
    "                f'{p[\"fault\"]}% {p[\"main_relay\"][\"relay\"]}/{p[\"backup_relay\"][\"relay\"]}',\n",
    "                abs(mt)\n",
    "            ])\n",
    "\n",
    "with open(OUT_CSV_SCEN,\"w\",newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"Escenario\",\"Pares evaluados\",\"Pares válidos\",\n",
    "                \"Descoordinados\",\"% Descoord\",\"TMT (negativo)\"])\n",
    "    for sid,d in sorted(per_scenario.items(),\n",
    "                        key=lambda k:int(k[0].split(\"_\")[-1])):\n",
    "        pct = 100*d[\"descoord\"]/d[\"valid\"] if d[\"valid\"] else 0\n",
    "        w.writerow([sid,d[\"total\"],d[\"valid\"],d[\"descoord\"],\n",
    "                    f\"{pct:.1f}\",f\"{d['TMT']:.5f}\"])\n",
    "print(\"✓ summary_scenarios.csv y mt_distribution.csv listos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Lee summary_scenarios.csv y genera:\n",
    "  • tmt_by_scenario.png  –  gráfica en barras del TMT (negativo)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Cargar CSV ---\n",
    "df = pd.read_csv(\"summary_scenarios_transformer.csv\")\n",
    "\n",
    "# --- 2. Extraer número de escenario y ordenar ---\n",
    "df[\"num\"] = pd.to_numeric(df[\"Escenario\"].str.extract(r\"(\\d+)\")[0])\n",
    "df = df.sort_values(\"num\")\n",
    "\n",
    "# --- 3. Graficar ---\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(df[\"Escenario\"], df[\"TMT (negativo)\"])\n",
    "plt.title(\"TMT por escenario (68 escenarios)\")\n",
    "plt.xlabel(\"Escenario\")\n",
    "plt.ylabel(\"TMT\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- 4. Guardar como PNG (300 dpi) ---\n",
    "plt.savefig(\"tmt_by_scenario_transformer.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✓ tmt_by_scenario_transformer.png generado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grap v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Lee TDS_Pickup_nuevos.csv con columnas:\n",
    "  escenario,par_rele,TDS_principal,Pickup_principal,TDS_respaldo,Pickup_respaldo\n",
    "Calcula TM = Pickup_respaldo − Pickup_principal − CTI\n",
    "y agrupa los TM negativos para obtener TMT por escenario.\n",
    "Dibuja un bar chart de TMT por escenario.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ─── Configuración de rutas y constantes ────────────────────────────────\n",
    "BASE   = Path(\"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/\")\n",
    "CSV_IN = BASE / \"data/processed/model/results/TDS_Pickup_nuevos.csv\"\n",
    "PNG_OUT = BASE / \"data/processed/model/results/tmt_by_scenario_prueba_v2.png\"\n",
    "CTI    = 0.2\n",
    "\n",
    "# ─── 1) Cargar CSV ───────────────────────────────────────────────────────\n",
    "df = pd.read_csv(CSV_IN)\n",
    "\n",
    "# ─── 2) Calcular TM (margen de tiempo usando pickup) ────────────────────\n",
    "# df[\"TM\"] = df[\"Pickup_respaldo\"] - df[\"Pickup_principal\"] - CTI\n",
    "#2) Calcular TM (margen de tiempo usando time out)\n",
    "\n",
    "\n",
    "# ─── 3) Agrupar para obtener TMT (suma de TM negativos) ─────────────────\n",
    "summary = (\n",
    "    df.groupby(\"escenario\")[\"TM\"]\n",
    "      .apply(lambda x: x[x < 0].sum())\n",
    "      .reset_index()\n",
    "      .rename(columns={\"TM\": \"TMT (negativo)\"})\n",
    ")\n",
    "\n",
    "# Extraer número de escenario para ordenarlo\n",
    "summary[\"num\"] = summary[\"escenario\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "summary = summary.sort_values(\"num\")\n",
    "\n",
    "# ─── 4) Gráfica de barras ───────────────────────────────────────────────\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(summary[\"escenario\"], summary[\"TMT (negativo)\"])\n",
    "plt.title(\"TMT por escenario (prueba)\")\n",
    "plt.xlabel(\"Escenario\")\n",
    "plt.ylabel(\"TMT (suma de márgenes negativos)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PNG_OUT, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gráfica guardada en: {PNG_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Lee los nuevos ajustes de TDS/pickup, recalcula Time_out con la función\n",
    "calculate_operation_time, obtiene delta_t y mt para cada par, agrupa\n",
    "los mt negativos en TMT por escenario y dibuja un bar chart.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────────────────────────── CONSTANTES ───────────────────────────\n",
    "K = 0.14\n",
    "N = 0.02\n",
    "DECIMAL_PLACES = 4\n",
    "CTI = 0.2\n",
    "\n",
    "def calculate_operation_time(I_shc, I_pi, TDS):\n",
    "    if not all(isinstance(x, (int, float)) for x in [I_shc, I_pi, TDS]):\n",
    "        return 0.0\n",
    "    if any(x is None or x <= 0 for x in [I_pi, TDS]) or I_shc <= 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        if abs(I_pi) < 1e-9:\n",
    "            return 0.0\n",
    "        M = I_shc / I_pi\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    if M <= 1:\n",
    "        return 0.0\n",
    "    try:\n",
    "        denominator = M**N - 1\n",
    "        if abs(denominator) < 1e-9:\n",
    "            return 0.0\n",
    "        timeout = (K / denominator) * TDS\n",
    "        return round(timeout, DECIMAL_PLACES) if np.isfinite(timeout) else 0.0\n",
    "    except (OverflowError, ValueError):\n",
    "        return 0.0\n",
    "\n",
    "# ─────────────────────────── RUTAS ───────────────────────────\n",
    "BASE    = Path(\"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/\")\n",
    "JSON_IN = BASE / \"data/processed/independent_relay_pairs_transformer.json\"\n",
    "CSV_IN  = BASE / \"data/processed/model/results/TDS_Pickup_nuevos.csv\"\n",
    "PNG_OUT = BASE / \"data/processed/model/results/tmt_by_scenario_prueba_v2.png\"\n",
    "\n",
    "# ────────────────── 1) Carga de pares originales ──────────────────\n",
    "with open(JSON_IN, \"r\") as f:\n",
    "    pairs = json.load(f)\n",
    "\n",
    "# Asignar par_rele en orden de aparición\n",
    "counts = {}\n",
    "for pair in pairs:\n",
    "    sid = pair[\"scenario_id\"]\n",
    "    counts[sid] = counts.get(sid, 0) + 1\n",
    "    pair[\"par_rele\"] = counts[sid]\n",
    "\n",
    "# ────────────────── 2) Leer CSV y construir mapa de ajustes ────────\n",
    "df_new = pd.read_csv(CSV_IN)\n",
    "new_map = {\n",
    "    (row[\"escenario\"], row[\"par_rele\"]): row.to_dict()\n",
    "    for _, row in df_new.iterrows()\n",
    "}\n",
    "\n",
    "# ────────────────── 3) Recalcular timeouts y márgenes ────────────\n",
    "for pair in pairs:\n",
    "    key = (pair[\"scenario_id\"], pair[\"par_rele\"])\n",
    "    ajustes = new_map.get(key)\n",
    "\n",
    "    main = pair[\"main_relay\"]\n",
    "    back = pair[\"backup_relay\"]\n",
    "\n",
    "    if ajustes:\n",
    "        main[\"TDS\"]     = ajustes[\"TDS_principal\"]\n",
    "        main[\"pick_up\"] = ajustes[\"Pickup_principal\"]\n",
    "        back[\"TDS\"]     = ajustes[\"TDS_respaldo\"]\n",
    "        back[\"pick_up\"] = ajustes[\"Pickup_respaldo\"]\n",
    "\n",
    "    main_time   = calculate_operation_time(main.get(\"Ishc\"), main.get(\"pick_up\"), main.get(\"TDS\"))\n",
    "    back_time   = calculate_operation_time(back.get(\"Ishc\"), back.get(\"pick_up\"), back.get(\"TDS\"))\n",
    "    pair[\"main_time\"]   = main_time\n",
    "    pair[\"backup_time\"] = back_time\n",
    "\n",
    "    delta_t = back_time - main_time - CTI\n",
    "    mt = (delta_t - abs(delta_t)) / 2\n",
    "    pair[\"delta_t\"] = delta_t\n",
    "    pair[\"mt\"]      = mt\n",
    "\n",
    "# ────────────────── 4) Agregar mt negativo a TMT por escenario ──────\n",
    "per_scenario = {}\n",
    "for p in pairs:\n",
    "    sid = p[\"scenario_id\"]\n",
    "    d = per_scenario.setdefault(sid, {\"TMT\": 0.0})\n",
    "    mt = p.get(\"mt\", 0.0)\n",
    "    if isinstance(mt, float) and mt < 0:\n",
    "        d[\"TMT\"] += mt\n",
    "\n",
    "# ────────────────── 5) Construir DataFrame de resumen ───────────────\n",
    "summary = [\n",
    "    {\"Escenario\": sid, \"TMT (negativo)\": vals[\"TMT\"]}\n",
    "    for sid, vals in sorted(per_scenario.items(),\n",
    "                            key=lambda x: int(x[0].split(\"_\")[-1]))\n",
    "]\n",
    "df_summary = pd.DataFrame(summary)\n",
    "\n",
    "# ────────────────── 6) Graficar TMT por escenario ────────────────────\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(df_summary[\"Escenario\"], df_summary[\"TMT (negativo)\"])\n",
    "plt.title(\"TMT por escenario (prueba v2)\")\n",
    "plt.xlabel(\"Escenario\")\n",
    "plt.ylabel(\"TMT (suma de márgenes negativos)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PNG_OUT, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gráfica guardada en: {PNG_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Calcula Time_out, delta_t y mt para cada par, suma los mt negativos\n",
    "en TMT por escenario y dibuja un bar chart interactivo con Plotly.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────────────────────────── CONSTANTES ───────────────────────────\n",
    "K = 0.14\n",
    "N = 0.02\n",
    "DECIMAL_PLACES = 4\n",
    "CTI = 0.2\n",
    "\n",
    "def calculate_operation_time(I_shc, I_pi, TDS):\n",
    "    if not all(isinstance(x, (int, float)) for x in (I_shc, I_pi, TDS)):\n",
    "        return 0.0\n",
    "    if any(x is None or x <= 0 for x in (I_pi, TDS)) or I_shc <= 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        if abs(I_pi) < 1e-9:\n",
    "            return 0.0\n",
    "        M = I_shc / I_pi\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    if M <= 1:\n",
    "        return 0.0\n",
    "    try:\n",
    "        denom = M**N - 1\n",
    "        if abs(denom) < 1e-9:\n",
    "            return 0.0\n",
    "        t = (K / denom) * TDS\n",
    "        return round(t, DECIMAL_PLACES) if np.isfinite(t) else 0.0\n",
    "    except (OverflowError, ValueError):\n",
    "        return 0.0\n",
    "\n",
    "# ─────────────────────────── RUTAS ───────────────────────────\n",
    "BASE    = Path(\"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/\")\n",
    "JSON_IN = BASE / \"data/processed/independent_relay_pairs_transformer.json\"\n",
    "CSV_IN  = BASE / \"data/processed/model/results/TDS_Pickup_nuevos.csv\"\n",
    "HTML_OUT = BASE / \"data/processed/model/results/tmt_by_scenario_prueba_v2.html\"\n",
    "PNG_OUT  = BASE / \"data/processed/model/results/tmt_by_scenario_prueba_v2.png\"\n",
    "\n",
    "# ────────────────── 1) Leer pares originales ──────────────────\n",
    "with open(JSON_IN, \"r\") as f:\n",
    "    pairs = json.load(f)\n",
    "\n",
    "# Asignar par_rele en orden de aparición\n",
    "counts = {}\n",
    "for pair in pairs:\n",
    "    sid = pair[\"scenario_id\"]\n",
    "    counts[sid] = counts.get(sid, 0) + 1\n",
    "    pair[\"par_rele\"] = counts[sid]\n",
    "\n",
    "# ──────────────── 2) Leer CSV de nuevos ajustes ───────────────\n",
    "df_new = pd.read_csv(CSV_IN)\n",
    "new_map = {\n",
    "    (int(r[\"escenario\"].split(\"_\")[-1]), r[\"par_rele\"]): r.to_dict()\n",
    "    for _, r in df_new.iterrows()\n",
    "}\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────────────────────────── CONSTANTES ───────────────────────────\n",
    "K = 0.14\n",
    "N = 0.02\n",
    "DECIMAL_PLACES = 4\n",
    "CTI = 0.2\n",
    "\n",
    "def calculate_operation_time(I_shc, I_pi, TDS):\n",
    "    if not all(isinstance(x, (int, float)) for x in (I_shc, I_pi, TDS)):\n",
    "        return 0.0\n",
    "    if any(x is None or x <= 0 for x in (I_pi, TDS)) or I_shc <= 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        if abs(I_pi) < 1e-9:\n",
    "            return 0.0\n",
    "        M = I_shc / I_pi\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    if M <= 1:\n",
    "        return 0.0\n",
    "    try:\n",
    "        denom = M**N - 1\n",
    "        if abs(denom) < 1e-9:\n",
    "            return 0.0\n",
    "        timeout = (K / denom) * TDS\n",
    "        return round(timeout, DECIMAL_PLACES) if np.isfinite(timeout) else 0.0\n",
    "    except (OverflowError, ValueError):\n",
    "        return 0.0\n",
    "\n",
    "# ─────────────────────────── RUTAS ───────────────────────────\n",
    "BASE    = Path(\"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/\")\n",
    "JSON_IN = BASE / \"data/processed/independent_relay_pairs_transformer.json\"\n",
    "CSV_IN  = BASE / \"data/processed/model/results/TDS_Pickup_nuevos.csv\"\n",
    "\n",
    "# ────────────────── 1) Leer pares originales ──────────────────\n",
    "with open(JSON_IN, \"r\") as f:\n",
    "    pairs = json.load(f)\n",
    "\n",
    "# Asignar par_rele en orden de aparición\n",
    "counts = {}\n",
    "for pair in pairs:\n",
    "    sid = pair[\"scenario_id\"]\n",
    "    counts[sid] = counts.get(sid, 0) + 1\n",
    "    pair[\"par_rele\"] = counts[sid]\n",
    "\n",
    "# ──────────────── 2) Leer CSV de nuevos ajustes ───────────────\n",
    "df_new = pd.read_csv(CSV_IN)\n",
    "new_map = {\n",
    "    (row[\"escenario\"], row[\"par_rele\"]): row.to_dict()\n",
    "    for _, row in df_new.iterrows()\n",
    "}\n",
    "\n",
    "# ──────── 3) Recalcular timeouts y márgenes y agregar mt ────────\n",
    "per_scenario = {}\n",
    "for pair in pairs:\n",
    "    key = (pair[\"scenario_id\"], pair[\"par_rele\"])\n",
    "    ajustes = new_map.get(key, {})\n",
    "\n",
    "    main = pair[\"main_relay\"]\n",
    "    back = pair[\"backup_relay\"]\n",
    "\n",
    "    # Actualizar configuraciones\n",
    "    if ajustes:\n",
    "        main[\"TDS\"]     = ajustes[\"TDS_principal\"]\n",
    "        main[\"pick_up\"] = ajustes[\"Pickup_principal\"]\n",
    "        back[\"TDS\"]     = ajustes[\"TDS_respaldo\"]\n",
    "        back[\"pick_up\"] = ajustes[\"Pickup_respaldo\"]\n",
    "\n",
    "    # Recalcular Time_out\n",
    "    mt_main = calculate_operation_time(main.get(\"Ishc\"), main.get(\"pick_up\"), main.get(\"TDS\"))\n",
    "    mt_back = calculate_operation_time(back.get(\"Ishc\"), back.get(\"pick_up\"), back.get(\"TDS\"))\n",
    "\n",
    "    # delta_t y mt\n",
    "    delta_t = mt_back - mt_main - CTI\n",
    "    mt_val  = (delta_t - abs(delta_t)) / 2\n",
    "\n",
    "    # Acumular TMT por escenario\n",
    "    d = per_scenario.setdefault(pair[\"scenario_id\"], 0.0)\n",
    "    if mt_val < 0:\n",
    "        per_scenario[pair[\"scenario_id\"]] += mt_val\n",
    "\n",
    "# ────────────────── 4) Preparar DataFrame de resumen ──────────────\n",
    "summary_df = pd.DataFrame([\n",
    "    {\"Escenario\": esc, \"TMT (negativo)\": tmt}\n",
    "    for esc, tmt in per_scenario.items()\n",
    "])\n",
    "# Ordenar por número\n",
    "summary_df[\"num\"] = summary_df[\"Escenario\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "summary_df = summary_df.sort_values(\"num\")\n",
    "\n",
    "# ────────────────── 5) Graficar con Plotly ───────────────────────\n",
    "fig = px.bar(\n",
    "    summary_df,\n",
    "    x=\"Escenario\",\n",
    "    y=\"TMT (negativo)\",\n",
    "    title=\"TMT por escenario (Plotly)\",\n",
    "    labels={\"TMT (negativo)\": \"TMT (suma márgenes negativos)\"}\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
