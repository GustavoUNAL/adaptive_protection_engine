{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb5a7f1",
   "metadata": {},
   "source": [
    "# Crea estructura de datos pares de relays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d492864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Leyendo ajustes de: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/results/TDS_Pickup_todos_escenarios.csv\n",
      "   Escenarios encontrados en CSV: ['scenario_1', 'scenario_2', 'scenario_3', 'scenario_4', 'scenario_5', 'scenario_6', 'scenario_7', 'scenario_8', 'scenario_9', 'scenario_10', 'scenario_11', 'scenario_12', 'scenario_13', 'scenario_14', 'scenario_15', 'scenario_16', 'scenario_17', 'scenario_18', 'scenario_19', 'scenario_20', 'scenario_21', 'scenario_22', 'scenario_23', 'scenario_24', 'scenario_25', 'scenario_26', 'scenario_27', 'scenario_28', 'scenario_29', 'scenario_30', 'scenario_31', 'scenario_32', 'scenario_33', 'scenario_34']\n",
      "▶ Leyendo coordinación base: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/raw/data_coordination.json\n",
      "  ⚠  Sin ajustes en CSV para scenario_35; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_36; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_37; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_38; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_39; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_40; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_41; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_42; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_43; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_44; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_45; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_46; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_47; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_48; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_49; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_50; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_51; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_52; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_53; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_54; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_55; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_56; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_57; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_58; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_59; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_60; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_61; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_62; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_63; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_64; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_65; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_66; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_67; omitido.\n",
      "  ⚠  Sin ajustes en CSV para scenario_68; omitido.\n",
      "▶ Pares generados: 3400\n",
      "✔ JSON guardado en: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/independent_relay_pairs_best.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Genera independent_relay_pairs_transformer.json\n",
    "leyendo los ajustes TDS / pickup de\n",
    "TDS_Pickup_todos_escenarios.csv, respetando\n",
    "el orden original de los renglones del CSV.\n",
    "\"\"\"\n",
    "\n",
    "import json, os, copy, csv, numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# ──────────────────────────── CONSTANTES ─────────────────────────────\n",
    "K, N, DEC = 0.14, 0.02, 4\n",
    "\n",
    "# ──────────────────────────── RUTAS ──────────────────────────────────\n",
    "BASE = \"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/\"\n",
    "\n",
    "DATA_COORD_FILE  = BASE + \"data/raw/data_coordination.json\"\n",
    "CSV_VALUES_FILE  = BASE + \"data/processed/model/results/TDS_Pickup_todos_escenarios.csv\"\n",
    "OUTPUT_PAIRS_FILE = BASE + \"data/processed/independent_relay_pairs_best.json\"\n",
    "\n",
    "# ──────────────────────────── AUXILIARES ─────────────────────────────\n",
    "def eliminar_timestamp(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        obj.pop(\"timestamp\", None)\n",
    "        for v in obj.values(): eliminar_timestamp(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for e in obj: eliminar_timestamp(e)\n",
    "\n",
    "def calc_timeout(Ishc, Ipi, TDS):\n",
    "    if not all(isinstance(x, (int, float)) for x in [Ishc, Ipi, TDS]):      return 0.0\n",
    "    if any(x is None or x <= 0 for x in [Ipi, TDS]) or Ishc <= Ipi:          return 0.0\n",
    "    try:\n",
    "        denom = (Ishc / Ipi) ** N - 1\n",
    "        if abs(denom) < 1e-9: return 0.0\n",
    "        t = (K / denom) * TDS\n",
    "        return round(t, DEC) if np.isfinite(t) else 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# ───────────────────── LEER CSV (en orden) ───────────────────────────\n",
    "print(f\"▶ Leyendo ajustes de: {CSV_VALUES_FILE}\")\n",
    "csv_queues = defaultdict(deque)      # {escenario: deque([row1,row2,...])}\n",
    "\n",
    "with open(CSV_VALUES_FILE, newline=\"\") as f:\n",
    "    rdr = csv.DictReader(f)\n",
    "    for row in rdr:\n",
    "        esc = row[\"escenario\"]                    # encabezado exacto\n",
    "        csv_queues[esc].append({\n",
    "            \"TDS_principal\":  float(row[\"TDS_principal\"]),\n",
    "            \"Pickup_princ\":   float(row[\"Pickup_principal\"]),\n",
    "            \"TDS_respaldo\":   float(row[\"TDS_respaldo\"]),\n",
    "            \"Pickup_resp\":    float(row[\"Pickup_respaldo\"]),\n",
    "        })\n",
    "\n",
    "print(f\"   Escenarios encontrados en CSV: {list(csv_queues.keys())}\")\n",
    "\n",
    "# ───────────────────── LEER JSON BASE ────────────────────────────────\n",
    "print(f\"▶ Leyendo coordinación base: {DATA_COORD_FILE}\")\n",
    "with open(DATA_COORD_FILE) as f:\n",
    "    base_json = json.load(f)\n",
    "\n",
    "processed_base = {}\n",
    "for esc in base_json:\n",
    "    sid = esc.get(\"scenario_id\")\n",
    "    if sid:\n",
    "        tmp = esc.copy()\n",
    "        tmp.pop(\"_id\", None); tmp.pop(\"scenario_id\", None)\n",
    "        eliminar_timestamp(tmp)\n",
    "        processed_base[sid] = tmp\n",
    "\n",
    "# ───────────────────── GENERAR LISTA DE PARES ────────────────────────\n",
    "relay_pairs, total_pairs = [], 0\n",
    "\n",
    "for sid, esc_data in processed_base.items():\n",
    "    if sid not in csv_queues:\n",
    "        print(f\"  ⚠  Sin ajustes en CSV para {sid}; omitido.\")\n",
    "        continue\n",
    "\n",
    "    for line_key, line_val in esc_data.items():\n",
    "        if not (isinstance(line_val, dict) and \"scenarios\" in line_val):\n",
    "            continue\n",
    "\n",
    "        for fault_key, fault_val in line_val[\"scenarios\"].items():\n",
    "            main_orig = fault_val.get(\"main\")\n",
    "            backups   = fault_val.get(\"backups\", [])\n",
    "\n",
    "            if not main_orig or not backups:      # seguridad\n",
    "                continue\n",
    "\n",
    "            main_base = copy.deepcopy(main_orig)\n",
    "            main_base[\"line\"] = line_key         # registrar línea\n",
    "\n",
    "            for bk_orig in backups:\n",
    "                if not isinstance(bk_orig, dict):\n",
    "                    continue\n",
    "\n",
    "                # ─── tomar siguiente renglón del CSV (orden original) ──\n",
    "                if not csv_queues[sid]:\n",
    "                    raise ValueError(f\"CSV sin suficientes filas para {sid}\")\n",
    "\n",
    "                ajustes = csv_queues[sid].popleft()\n",
    "\n",
    "                main_info = copy.deepcopy(main_base)\n",
    "                bk_info   = copy.deepcopy(bk_orig)\n",
    "\n",
    "                # aplicar ajustes principal / respaldo\n",
    "                main_info[\"TDS\"]     = ajustes[\"TDS_principal\"]\n",
    "                main_info[\"pick_up\"] = ajustes[\"Pickup_princ\"]\n",
    "                bk_info[\"TDS\"]       = ajustes[\"TDS_respaldo\"]\n",
    "                bk_info[\"pick_up\"]   = ajustes[\"Pickup_resp\"]\n",
    "\n",
    "                # recalcular Time_out\n",
    "                main_info[\"Time_out\"] = calc_timeout(\n",
    "                    main_info.get(\"Ishc\"), main_info.get(\"pick_up\"), main_info.get(\"TDS\")\n",
    "                )\n",
    "                bk_info[\"Time_out\"] = calc_timeout(\n",
    "                    bk_info.get(\"Ishc\"), bk_info.get(\"pick_up\"), bk_info.get(\"TDS\")\n",
    "                )\n",
    "\n",
    "                relay_pairs.append({\n",
    "                    \"scenario_id\": sid,\n",
    "                    \"fault\": fault_key,\n",
    "                    \"main_relay\": main_info,\n",
    "                    \"backup_relay\": bk_info,\n",
    "                })\n",
    "                total_pairs += 1\n",
    "\n",
    "print(f\"▶ Pares generados: {total_pairs}\")\n",
    "\n",
    "# ───────────────────── GUARDAR RESULTADO ─────────────────────────────\n",
    "os.makedirs(os.path.dirname(OUTPUT_PAIRS_FILE), exist_ok=True)\n",
    "with open(OUTPUT_PAIRS_FILE, \"w\") as f:\n",
    "    json.dump(relay_pairs, f, indent=2)\n",
    "\n",
    "print(f\"✔ JSON guardado en: {OUTPUT_PAIRS_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee70d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974b078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e437f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952d48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos base desde: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/raw/data_coordination.json\n",
      "Datos base cargados y pre-procesados para 68 escenarios.\n",
      "Cargando valores de relés desde: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/raw/optimized_relay_values.json\n",
      "Valores de relés cargados para 68 escenarios.\n",
      "Procesando escenarios para crear pares de relés independientes...\n",
      "Procesando escenario: scenario_1\n",
      "Procesando escenario: scenario_2\n",
      "Procesando escenario: scenario_3\n",
      "Procesando escenario: scenario_4\n",
      "Procesando escenario: scenario_5\n",
      "Procesando escenario: scenario_6\n",
      "Procesando escenario: scenario_7\n",
      "Procesando escenario: scenario_8\n",
      "Procesando escenario: scenario_9\n",
      "Procesando escenario: scenario_10\n",
      "Procesando escenario: scenario_11\n",
      "Procesando escenario: scenario_12\n",
      "Procesando escenario: scenario_13\n",
      "Procesando escenario: scenario_14\n",
      "Procesando escenario: scenario_15\n",
      "Procesando escenario: scenario_16\n",
      "Procesando escenario: scenario_17\n",
      "Procesando escenario: scenario_18\n",
      "Procesando escenario: scenario_19\n",
      "Procesando escenario: scenario_20\n",
      "Procesando escenario: scenario_21\n",
      "Procesando escenario: scenario_22\n",
      "Procesando escenario: scenario_23\n",
      "Procesando escenario: scenario_24\n",
      "Procesando escenario: scenario_25\n",
      "Procesando escenario: scenario_26\n",
      "Procesando escenario: scenario_27\n",
      "Procesando escenario: scenario_28\n",
      "Procesando escenario: scenario_29\n",
      "Procesando escenario: scenario_30\n",
      "Procesando escenario: scenario_31\n",
      "Procesando escenario: scenario_32\n",
      "Procesando escenario: scenario_33\n",
      "Procesando escenario: scenario_34\n",
      "Procesando escenario: scenario_35\n",
      "Procesando escenario: scenario_36\n",
      "Procesando escenario: scenario_37\n",
      "Procesando escenario: scenario_38\n",
      "Procesando escenario: scenario_39\n",
      "Procesando escenario: scenario_40\n",
      "Procesando escenario: scenario_41\n",
      "Procesando escenario: scenario_42\n",
      "Procesando escenario: scenario_43\n",
      "Procesando escenario: scenario_44\n",
      "Procesando escenario: scenario_45\n",
      "Procesando escenario: scenario_46\n",
      "Procesando escenario: scenario_47\n",
      "Procesando escenario: scenario_48\n",
      "Procesando escenario: scenario_49\n",
      "Procesando escenario: scenario_50\n",
      "Procesando escenario: scenario_51\n",
      "Procesando escenario: scenario_52\n",
      "Procesando escenario: scenario_53\n",
      "Procesando escenario: scenario_54\n",
      "Procesando escenario: scenario_55\n",
      "Procesando escenario: scenario_56\n",
      "Procesando escenario: scenario_57\n",
      "Procesando escenario: scenario_58\n",
      "Procesando escenario: scenario_59\n",
      "Procesando escenario: scenario_60\n",
      "Procesando escenario: scenario_61\n",
      "Procesando escenario: scenario_62\n",
      "Procesando escenario: scenario_63\n",
      "Procesando escenario: scenario_64\n",
      "Procesando escenario: scenario_65\n",
      "Procesando escenario: scenario_66\n",
      "Procesando escenario: scenario_67\n",
      "Procesando escenario: scenario_68\n",
      "Procesamiento completado. Se generaron 6800 pares de relés.\n",
      "Guardando lista de pares en: /Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/independent_relay_pairs.json\n",
      "¡Proceso completado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import copy # Para crear copias independientes de los diccionarios de relés\n",
    "\n",
    "# --- Constantes para el cálculo de Time_out ---\n",
    "K = 0.14\n",
    "N = 0.02\n",
    "DECIMAL_PLACES = 4\n",
    "\n",
    "# --- Rutas de los archivos ---\n",
    "data_coordination_file = \"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/raw/data_coordination.json\"\n",
    "# datos de los reusltados de la red neuronal\n",
    "relay_values_file = \"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/model/results/TDS_Pickup_todos_escenarios.csv\"\n",
    "# El archivo de salida contendrá la LISTA de pares con la nueva estructura\n",
    "output_pairs_file = \"/Users/gustavo/Documents/Projects/TESIS_UNAL/ADAPTIVE_ALGORITHM/data/processed/independent_relay_pairs_transformer.json\" # Nuevo nombre para evitar sobreescribir\n",
    "\n",
    "# --- Función para eliminar timestamps recursivamente ---\n",
    "def eliminar_timestamp(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        obj.pop(\"timestamp\", None)\n",
    "        for valor in obj.values():\n",
    "            eliminar_timestamp(valor)\n",
    "    elif isinstance(obj, list):\n",
    "        for elemento in obj:\n",
    "            eliminar_timestamp(elemento)\n",
    "\n",
    "# --- Función para calcular el tiempo de operación (Time_out) ---\n",
    "def calculate_operation_time(I_shc, I_pi, TDS):\n",
    "    # (Sin cambios en la función de cálculo)\n",
    "    if not all(isinstance(x, (int, float)) for x in [I_shc, I_pi, TDS]): return 0.0\n",
    "    if any(x is None or x <= 0 for x in [I_pi, TDS]): return 0.0\n",
    "    if I_shc < 0: return 0.0\n",
    "    if I_shc == 0: return 0.0\n",
    "    try:\n",
    "        if abs(I_pi) < 1e-9: return 0.0\n",
    "        M = I_shc / I_pi\n",
    "    except ZeroDivisionError: return 0.0\n",
    "    if M <= 1: return 0.0\n",
    "    try:\n",
    "        denominator = M**N - 1\n",
    "        if abs(denominator) < 1e-9: return 0.0\n",
    "        timeout = (K / denominator) * TDS\n",
    "        return round(timeout, DECIMAL_PLACES) if np.isfinite(timeout) else 0.0\n",
    "    except (OverflowError, ValueError): return 0.0\n",
    "    except Exception: return 0.0\n",
    "\n",
    "\n",
    "# --- Procesamiento Principal ---\n",
    "relay_pairs_list = []\n",
    "processed_scenarios_base = {}\n",
    "\n",
    "try:\n",
    "    # 1. Cargar y pre-procesar los datos base de coordinación\n",
    "    print(f\"Cargando datos base desde: {data_coordination_file}\")\n",
    "    with open(data_coordination_file, 'r') as archivo:\n",
    "        datos_coordinacion = json.load(archivo)\n",
    "\n",
    "    for escenario_base in datos_coordinacion:\n",
    "        scenario_id = escenario_base.get(\"scenario_id\")\n",
    "        if scenario_id:\n",
    "            escenario_procesado = escenario_base.copy()\n",
    "            escenario_procesado.pop(\"_id\", None)\n",
    "            escenario_procesado.pop(\"scenario_id\", None)\n",
    "            eliminar_timestamp(escenario_procesado)\n",
    "            processed_scenarios_base[scenario_id] = escenario_procesado\n",
    "    print(f\"Datos base cargados y pre-procesados para {len(processed_scenarios_base)} escenarios.\")\n",
    "\n",
    "    # 2. Cargar los valores de los relés (antes 'optimizados')\n",
    "    print(f\"Cargando valores de relés desde: {relay_values_file}\")\n",
    "    with open(relay_values_file, 'r') as archivo:\n",
    "        # Renombrado: datos_relay_values\n",
    "        datos_relay_values = json.load(archivo)\n",
    "    # Renombrado: relay_values_map\n",
    "    # ASUNCIÓN: La clave DENTRO del JSON ahora es 'relay_values'.\n",
    "    # Si sigue siendo 'optimized_relay_values', cambia el .get() abajo.\n",
    "    relay_values_map = {esc.get(\"scenario_id\"): esc.get(\"relay_values\", {})\n",
    "                        for esc in datos_relay_values if esc.get(\"scenario_id\")}\n",
    "    print(f\"Valores de relés cargados para {len(relay_values_map)} escenarios.\")\n",
    "\n",
    "    # 3. Fusionar, calcular timeouts y CONSTRUIR LISTA DE PARES (con línea en main_relay)\n",
    "    print(\"Procesando escenarios para crear pares de relés independientes...\")\n",
    "    pair_count = 0\n",
    "    for scenario_id, escenario_data in processed_scenarios_base.items():\n",
    "        print(f\"Procesando escenario: {scenario_id}\")\n",
    "        # Renombrado: current_relay_values\n",
    "        current_relay_values = relay_values_map.get(scenario_id)\n",
    "\n",
    "        if not current_relay_values:\n",
    "            print(f\"  Advertencia: No se encontraron valores de relé para {scenario_id}. Omitiendo pares de este escenario.\")\n",
    "            continue\n",
    "\n",
    "        # Iterar sobre las líneas (como \"L1-2\")\n",
    "        for linea_key, linea_data in escenario_data.items():\n",
    "             if isinstance(linea_data, dict) and 'scenarios' in linea_data:\n",
    "                # Iterar sobre los escenarios internos/fallas (como \"90\")\n",
    "                for fault_key, internal_scenario_data in linea_data.get('scenarios', {}).items():\n",
    "                    main_relay_info_orig = internal_scenario_data.get('main')\n",
    "                    backups_orig = internal_scenario_data.get('backups', [])\n",
    "\n",
    "                    if isinstance(main_relay_info_orig, dict) and isinstance(backups_orig, list):\n",
    "                        # Procesar relé principal UNA VEZ\n",
    "                        main_relay_info = copy.deepcopy(main_relay_info_orig)\n",
    "                        main_relay_name = main_relay_info.get('relay')\n",
    "                        main_time_out = 0.0\n",
    "\n",
    "                        # --- Añadir la línea principal AL OBJETO main_relay ---\n",
    "                        main_relay_info['line'] = linea_key\n",
    "                        # ----------------------------------------------------\n",
    "\n",
    "                        # Renombrado: current_relay_values\n",
    "                        if main_relay_name and main_relay_name in current_relay_values:\n",
    "                            # Renombrado: relay_setting\n",
    "                            relay_setting = current_relay_values[main_relay_name]\n",
    "                            main_relay_info['TDS'] = relay_setting.get('TDS')\n",
    "                            main_relay_info['pick_up'] = relay_setting.get('pickup')\n",
    "                            ishc = main_relay_info.get('Ishc')\n",
    "                            pickup = main_relay_info.get('pick_up')\n",
    "                            tds = main_relay_info.get('TDS')\n",
    "                            main_time_out = calculate_operation_time(ishc, pickup, tds)\n",
    "                            main_relay_info['Time_out'] = main_time_out\n",
    "                        else:\n",
    "                             ishc = main_relay_info.get('Ishc')\n",
    "                             pickup = main_relay_info.get('pick_up')\n",
    "                             tds = main_relay_info.get('TDS')\n",
    "                             main_time_out = calculate_operation_time(ishc, pickup, tds)\n",
    "                             main_relay_info['Time_out'] = main_time_out\n",
    "\n",
    "                        # Iterar sobre CADA relé de respaldo\n",
    "                        for backup_relay_info_orig in backups_orig:\n",
    "                            if isinstance(backup_relay_info_orig, dict):\n",
    "                                backup_relay_info = copy.deepcopy(backup_relay_info_orig)\n",
    "                                backup_relay_name = backup_relay_info.get('relay')\n",
    "                                backup_time_out = 0.0\n",
    "\n",
    "                                # Renombrado: current_relay_values\n",
    "                                if backup_relay_name and backup_relay_name in current_relay_values:\n",
    "                                    # Renombrado: relay_setting\n",
    "                                    relay_setting = current_relay_values[backup_relay_name]\n",
    "                                    backup_relay_info['TDS'] = relay_setting.get('TDS')\n",
    "                                    backup_relay_info['pick_up'] = relay_setting.get('pickup')\n",
    "                                    ishc = backup_relay_info.get('Ishc')\n",
    "                                    pickup = backup_relay_info.get('pick_up')\n",
    "                                    tds = backup_relay_info.get('TDS')\n",
    "                                    backup_time_out = calculate_operation_time(ishc, pickup, tds)\n",
    "                                    backup_relay_info['Time_out'] = backup_time_out\n",
    "                                else:\n",
    "                                    ishc = backup_relay_info.get('Ishc')\n",
    "                                    pickup = backup_relay_info.get('pick_up')\n",
    "                                    tds = backup_relay_info.get('TDS')\n",
    "                                    backup_time_out = calculate_operation_time(ishc, pickup, tds)\n",
    "                                    backup_relay_info['Time_out'] = backup_time_out\n",
    "\n",
    "                                # Crear el diccionario del par (SIN 'line' de nivel superior)\n",
    "                                pair_entry = {\n",
    "                                    \"scenario_id\": scenario_id,\n",
    "                                    # \"line\": linea_key, # <-- Eliminado de aquí\n",
    "                                    \"fault\": fault_key,\n",
    "                                    \"main_relay\": main_relay_info, # Ya contiene 'line'\n",
    "                                    \"backup_relay\": backup_relay_info # Mantiene su 'line' interna opcional\n",
    "                                }\n",
    "                                relay_pairs_list.append(pair_entry)\n",
    "                                pair_count += 1\n",
    "\n",
    "    print(f\"Procesamiento completado. Se generaron {pair_count} pares de relés.\")\n",
    "\n",
    "    # 4. Guardar la LISTA de pares en el archivo de salida\n",
    "    print(f\"Guardando lista de pares en: {output_pairs_file}\")\n",
    "    output_dir = os.path.dirname(output_pairs_file)\n",
    "    if output_dir: os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(output_pairs_file, 'w') as f:\n",
    "        json.dump(relay_pairs_list, f, indent=2)\n",
    "\n",
    "    print(\"¡Proceso completado exitosamente!\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error CRÍTICO: No se pudo encontrar el archivo: {e.filename}\")\n",
    "    print(\"Verifica que las rutas y nombres de archivo sean correctos (especialmente 'relay_values.json').\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error CRÍTICO: El archivo JSON está mal formado: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Falta una clave esperada en los datos: {e}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error inesperado durante el procesamiento: {e}\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
