{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear  Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:55:29,977] A new study created in memory with name: no-name-5c62e5bb-d4e6-4d6e-b7d5-3d40f681cb4a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando y preprocesando datos...\n",
      "Datos de ejemplo generados: input_shape=torch.Size([500, 74, 6]), target_shape=torch.Size([500, 74, 4])\n",
      "Scalers guardados como 'scaler_input.pkl' y 'scaler_target.pkl'.\n",
      "Datos divididos: 400 entrenamiento, 100 validación.\n",
      "\n",
      "--- Iniciando Optimización de Hiperparámetros (Optuna) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.278221, Val Loss: 1.275360\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.275360)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.219781)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.182286)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.155614)\n",
      "Epoch 5/25, Train Loss: 1.167438, Val Loss: 1.135022\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.135022)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.118492)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.105050)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.094062)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.084808)\n",
      "Epoch 10/25, Train Loss: 1.121414, Val Loss: 1.076828\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.076828)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.070486)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.065035)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.060053)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.056002)\n",
      "Epoch 15/25, Train Loss: 1.096563, Val Loss: 1.052336\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.052336)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.049095)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.046668)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.044203)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.042236)\n",
      "Epoch 20/25, Train Loss: 1.078091, Val Loss: 1.040442\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.040442)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.038716)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.037243)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.035914)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.034754)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:55:59,186] Trial 0 finished with value: 1.0334512761660986 and parameters: {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 3, 'dim_feedforward': 128, 'dropout': 0.39236353069604846, 'learning_rate': 1.166863683915956e-05, 'batch_size': 16, 'weight_decay': 0.0006136698219984957}. Best is trial 0 with value: 1.0334512761660986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.071557, Val Loss: 1.033451\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_0.pth (Val Loss: 1.033451)\n",
      "Entrenamiento completado. Mejor Val Loss: 1.033451\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.441508, Val Loss: 1.313603\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.313603)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.208074)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.149750)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.104083)\n",
      "Epoch 5/25, Train Loss: 1.133650, Val Loss: 1.077888\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.077888)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.063712)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.054056)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.047300)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.042172)\n",
      "Epoch 10/25, Train Loss: 1.082857, Val Loss: 1.038990\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.038990)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.036004)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.034347)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.032793)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.031278)\n",
      "Epoch 15/25, Train Loss: 1.060354, Val Loss: 1.030117\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.030117)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.029518)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.028782)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.027788)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.027298)\n",
      "Epoch 20/25, Train Loss: 1.051518, Val Loss: 1.027084\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.027084)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.026408)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.025674)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.025634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:56:50,705] Trial 1 finished with value: 1.0254742801189423 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 128, 'dropout': 0.372950366621266, 'learning_rate': 2.4929354146798223e-05, 'batch_size': 32, 'weight_decay': 1.834679339518728e-05}. Best is trial 1 with value: 1.0254742801189423.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.043442, Val Loss: 1.025474\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_1.pth (Val Loss: 1.025474)\n",
      "Entrenamiento completado. Mejor Val Loss: 1.025474\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.198510, Val Loss: 1.124361\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.124361)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.073541)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.049892)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.040633)\n",
      "Epoch 5/25, Train Loss: 1.046632, Val Loss: 1.035934\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.035934)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.033137)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.031226)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.029765)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.028752)\n",
      "Epoch 10/25, Train Loss: 1.032794, Val Loss: 1.027926\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.027926)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.027356)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.026551)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.026242)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.025668)\n",
      "Epoch 15/25, Train Loss: 1.023091, Val Loss: 1.025310\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.025310)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.025080)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.024990)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.024656)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.024425)\n",
      "Epoch 20/25, Train Loss: 1.018701, Val Loss: 1.024391\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.024391)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.024156)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.024110)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.023792)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:57:19,833] Trial 2 finished with value: 1.0235818922519684 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 3, 'dim_feedforward': 256, 'dropout': 0.21088795059542947, 'learning_rate': 2.785158719618567e-05, 'batch_size': 32, 'weight_decay': 0.00020342805385118883}. Best is trial 2 with value: 1.0235818922519684.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.016206, Val Loss: 1.023582\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_2.pth (Val Loss: 1.023582)\n",
      "Entrenamiento completado. Mejor Val Loss: 1.023582\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.111440, Val Loss: 1.041297\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.041297)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.027573)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.023773)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.022662)\n",
      "Epoch 5/25, Train Loss: 1.014887, Val Loss: 1.022320\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.022320)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.022046)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.021937)\n",
      "Epoch 10/25, Train Loss: 1.007075, Val Loss: 1.022354\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.021834)\n",
      "Epoch 15/25, Train Loss: 1.004816, Val Loss: 1.021647\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.021647)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_3.pth (Val Loss: 1.021397)\n",
      "Epoch 20/25, Train Loss: 1.002009, Val Loss: 1.021667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:57:45,674] Trial 3 finished with value: 1.0213970243930817 and parameters: {'d_model': 32, 'nhead': 4, 'num_encoder_layers': 3, 'dim_feedforward': 256, 'dropout': 0.29205785019056346, 'learning_rate': 0.0003212539417978877, 'batch_size': 32, 'weight_decay': 6.349460371792825e-06}. Best is trial 3 with value: 1.0213970243930817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.001477, Val Loss: 1.021482\n",
      "Entrenamiento completado. Mejor Val Loss: 1.021397\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.212603, Val Loss: 1.141739\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.141739)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.076868)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.049850)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.038219)\n",
      "Epoch 5/25, Train Loss: 1.044538, Val Loss: 1.032652\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.032652)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.029502)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.027535)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.026323)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.025307)\n",
      "Epoch 10/25, Train Loss: 1.025776, Val Loss: 1.024669\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.024669)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.024272)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.023657)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.023414)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.023195)\n",
      "Epoch 15/25, Train Loss: 1.017926, Val Loss: 1.023002\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.023002)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.022760)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.022523)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.022486)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.022389)\n",
      "Epoch 20/25, Train Loss: 1.014096, Val Loss: 1.022312\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.022312)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.022203)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_4.pth (Val Loss: 1.022038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:58:31,603] Trial 4 finished with value: 1.0220377147197723 and parameters: {'d_model': 32, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 128, 'dropout': 0.30494325403553635, 'learning_rate': 0.00012629483952232367, 'batch_size': 32, 'weight_decay': 1.7486263433640494e-05}. Best is trial 3 with value: 1.0213970243930817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.009652, Val Loss: 1.022058\n",
      "Entrenamiento completado. Mejor Val Loss: 1.022038\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.237843, Val Loss: 1.179274\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.179274)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.116882)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.082825)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.062450)\n",
      "Epoch 5/25, Train Loss: 1.086283, Val Loss: 1.049559\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.049559)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.041171)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.035520)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.032123)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.029394)\n",
      "Epoch 10/25, Train Loss: 1.052795, Val Loss: 1.027449\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.027449)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.026020)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.024937)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.024029)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.023190)\n",
      "Epoch 15/25, Train Loss: 1.039206, Val Loss: 1.022529\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.022529)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.022046)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.021446)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.021079)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.020774)\n",
      "Epoch 20/25, Train Loss: 1.034064, Val Loss: 1.020501\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.020501)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.020109)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.020003)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.019813)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.019608)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:58:58,709] Trial 5 finished with value: 1.0194911871637617 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.30064888186718963, 'learning_rate': 2.1283464050364642e-05, 'batch_size': 16, 'weight_decay': 0.00010177359051613628}. Best is trial 5 with value: 1.0194911871637617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.028582, Val Loss: 1.019491\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_5.pth (Val Loss: 1.019491)\n",
      "Entrenamiento completado. Mejor Val Loss: 1.019491\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.123133, Val Loss: 1.045093\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.045093)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.025055)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.023045)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.022795)\n",
      "Epoch 5/25, Train Loss: 1.012846, Val Loss: 1.022467\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.022467)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.022361)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.021659)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.021476)\n",
      "Epoch 10/25, Train Loss: 1.003806, Val Loss: 1.021991\n",
      "Epoch 15/25, Train Loss: 1.001855, Val Loss: 1.021321\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_6.pth (Val Loss: 1.021321)\n",
      "Epoch 20/25, Train Loss: 0.998638, Val Loss: 1.022252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:59:36,136] Trial 6 finished with value: 1.0213211476802826 and parameters: {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 128, 'dropout': 0.3011309973389743, 'learning_rate': 0.0007890950788164766, 'batch_size': 32, 'weight_decay': 2.2173963930491094e-05}. Best is trial 5 with value: 1.0194911871637617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 0.999615, Val Loss: 1.021532\n",
      "Early stopping en época 25 después de 10 épocas sin mejora.\n",
      "Entrenamiento completado. Mejor Val Loss: 1.021321\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.204821, Val Loss: 1.091574\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.091574)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.054678)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.041177)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.035957)\n",
      "Epoch 5/25, Train Loss: 1.054001, Val Loss: 1.032778\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.032778)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.029580)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.027210)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.026832)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.026702)\n",
      "Epoch 10/25, Train Loss: 1.033149, Val Loss: 1.025641\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.025641)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.024561)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.024423)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.023940)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.023341)\n",
      "Epoch 15/25, Train Loss: 1.025121, Val Loss: 1.023165\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.023165)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.022734)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.022691)\n",
      "Epoch 20/25, Train Loss: 1.019213, Val Loss: 1.023033\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_7.pth (Val Loss: 1.022591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:59:55,778] Trial 7 finished with value: 1.0225909352302551 and parameters: {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 1, 'dim_feedforward': 128, 'dropout': 0.2887055897285994, 'learning_rate': 9.89064649004482e-05, 'batch_size': 32, 'weight_decay': 1.2198076241304423e-06}. Best is trial 5 with value: 1.0194911871637617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.015184, Val Loss: 1.023051\n",
      "Entrenamiento completado. Mejor Val Loss: 1.022591\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.293167, Val Loss: 1.250568\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.250568)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.162102)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.112869)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.085898)\n",
      "Epoch 5/25, Train Loss: 1.084246, Val Loss: 1.070967\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.070967)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.061482)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.055367)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.051423)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.048485)\n",
      "Epoch 10/25, Train Loss: 1.053739, Val Loss: 1.046011\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.046011)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.043946)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.042121)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.040554)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.038915)\n",
      "Epoch 15/25, Train Loss: 1.038858, Val Loss: 1.037500\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.037500)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.036391)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.035443)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.034130)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.033588)\n",
      "Epoch 20/25, Train Loss: 1.031168, Val Loss: 1.032935\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.032935)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.032342)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.031697)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.031177)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.030607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:00:19,397] Trial 8 finished with value: 1.0302011966705322 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 1, 'dim_feedforward': 128, 'dropout': 0.2009596579510352, 'learning_rate': 3.4761542134006376e-05, 'batch_size': 32, 'weight_decay': 0.0008275399810372122}. Best is trial 5 with value: 1.0194911871637617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.025322, Val Loss: 1.030201\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_8.pth (Val Loss: 1.030201)\n",
      "Entrenamiento completado. Mejor Val Loss: 1.030201\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.209409, Val Loss: 1.122780\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.122780)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.058957)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.040446)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.033115)\n",
      "Epoch 5/25, Train Loss: 1.034453, Val Loss: 1.029895\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.029895)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.028081)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.027012)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.026036)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.025379)\n",
      "Epoch 10/25, Train Loss: 1.019170, Val Loss: 1.024758\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.024758)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.024370)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.023890)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.023555)\n",
      "Epoch 15/25, Train Loss: 1.010649, Val Loss: 1.023244\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.023244)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.023174)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.022771)\n",
      "Epoch 20/25, Train Loss: 1.009257, Val Loss: 1.022799\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_9.pth (Val Loss: 1.022411)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:00:51,380] Trial 9 finished with value: 1.0224107205867767 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 2, 'dim_feedforward': 128, 'dropout': 0.1576004451001064, 'learning_rate': 0.00011800955360567709, 'batch_size': 32, 'weight_decay': 3.6458914840540794e-06}. Best is trial 5 with value: 1.0194911871637617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.006603, Val Loss: 1.022525\n",
      "Entrenamiento completado. Mejor Val Loss: 1.022411\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.291059, Val Loss: 1.252848\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.252848)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.177430)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.130250)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.099737)\n",
      "Epoch 5/25, Train Loss: 1.091738, Val Loss: 1.079586\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.079586)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.065859)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.056445)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.049767)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.044939)\n",
      "Epoch 10/25, Train Loss: 1.046007, Val Loss: 1.041260\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.041260)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.038309)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.035990)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.034169)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.032500)\n",
      "Epoch 15/25, Train Loss: 1.032337, Val Loss: 1.031234\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.031234)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.030092)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.029023)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.028074)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.027284)\n",
      "Epoch 20/25, Train Loss: 1.026624, Val Loss: 1.026676\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.026676)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.026000)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.025333)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.024860)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.024453)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:01:19,909] Trial 10 finished with value: 1.0240049021584647 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 512, 'dropout': 0.1276038311254491, 'learning_rate': 1.10695364164496e-05, 'batch_size': 16, 'weight_decay': 0.00010272445305765346}. Best is trial 5 with value: 1.0194911871637617.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 1.021730, Val Loss: 1.024005\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_10.pth (Val Loss: 1.024005)\n",
      "Entrenamiento completado. Mejor Val Loss: 1.024005\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.077375, Val Loss: 1.021269\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_11.pth (Val Loss: 1.021269)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_11.pth (Val Loss: 1.017812)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_11.pth (Val Loss: 1.016827)\n",
      "Epoch 5/25, Train Loss: 1.005968, Val Loss: 1.017658\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_11.pth (Val Loss: 1.016469)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_11.pth (Val Loss: 1.016141)\n",
      "Epoch 10/25, Train Loss: 1.000962, Val Loss: 1.016891\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_11.pth (Val Loss: 1.015639)\n",
      "Epoch 15/25, Train Loss: 0.998791, Val Loss: 1.016638\n",
      "Epoch 20/25, Train Loss: 0.998326, Val Loss: 1.016782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:01:36,637] Trial 11 finished with value: 1.0156389815466744 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.3363087097284188, 'learning_rate': 0.0008921043940918774, 'batch_size': 16, 'weight_decay': 7.547118515409804e-05}. Best is trial 11 with value: 1.0156389815466744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping en época 21 después de 10 épocas sin mejora.\n",
      "Entrenamiento completado. Mejor Val Loss: 1.015639\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.071913, Val Loss: 1.021331\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_12.pth (Val Loss: 1.021331)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_12.pth (Val Loss: 1.019374)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_12.pth (Val Loss: 1.018142)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_12.pth (Val Loss: 1.017273)\n",
      "Epoch 5/25, Train Loss: 1.004081, Val Loss: 1.017283\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_12.pth (Val Loss: 1.016847)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_12.pth (Val Loss: 1.016133)\n",
      "Epoch 10/25, Train Loss: 0.999616, Val Loss: 1.016022\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_12.pth (Val Loss: 1.016022)\n",
      "Epoch 15/25, Train Loss: 0.999082, Val Loss: 1.017642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:01:55,613] Trial 12 finished with value: 1.0160219073295593 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.36571702071861206, 'learning_rate': 0.0009261956215494404, 'batch_size': 16, 'weight_decay': 0.00010439090426743462}. Best is trial 11 with value: 1.0156389815466744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Train Loss: 0.998494, Val Loss: 1.016771\n",
      "Early stopping en época 20 después de 10 épocas sin mejora.\n",
      "Entrenamiento completado. Mejor Val Loss: 1.016022\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.086991, Val Loss: 1.022012\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.022012)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.019585)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.019294)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.018134)\n",
      "Epoch 5/25, Train Loss: 1.007328, Val Loss: 1.017402\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.017402)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.016999)\n",
      "Epoch 10/25, Train Loss: 1.001192, Val Loss: 1.016822\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.016822)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.016731)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_13.pth (Val Loss: 1.016424)\n",
      "Epoch 15/25, Train Loss: 0.999336, Val Loss: 1.016847\n",
      "Epoch 20/25, Train Loss: 0.998591, Val Loss: 1.017095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:02:06,737] Trial 13 finished with value: 1.0164240172931127 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 1, 'dim_feedforward': 256, 'dropout': 0.3450950267573669, 'learning_rate': 0.0009699925671540091, 'batch_size': 16, 'weight_decay': 0.00010614982403330936}. Best is trial 11 with value: 1.0156389815466744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping en época 24 después de 10 épocas sin mejora.\n",
      "Entrenamiento completado. Mejor Val Loss: 1.016424\n",
      "\n",
      "Iniciando entrenamiento por 25 épocas...\n",
      "Epoch 1/25, Train Loss: 1.128899, Val Loss: 1.025213\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.025213)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.019839)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.018324)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.017644)\n",
      "Epoch 5/25, Train Loss: 1.014119, Val Loss: 1.017421\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.017421)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.017273)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.016583)\n",
      "Epoch 10/25, Train Loss: 1.005280, Val Loss: 1.016550\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.016550)\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.016168)\n",
      "Epoch 15/25, Train Loss: 1.001649, Val Loss: 1.015970\n",
      "---> Nuevo mejor modelo guardado en temp_best_model_trial_14.pth (Val Loss: 1.015970)\n",
      "Epoch 20/25, Train Loss: 0.999395, Val Loss: 1.016831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 17:02:27,390] Trial 14 finished with value: 1.0159698128700256 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.3475639849474677, 'learning_rate': 0.000436333434844535, 'batch_size': 16, 'weight_decay': 0.0002762698242066277}. Best is trial 11 with value: 1.0156389815466744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train Loss: 0.998609, Val Loss: 1.016343\n",
      "Early stopping en época 25 después de 10 épocas sin mejora.\n",
      "Entrenamiento completado. Mejor Val Loss: 1.015970\n",
      "\n",
      "--- Mejores Hiperparámetros Encontrados ---\n",
      "{'d_model': 64, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.3363087097284188, 'learning_rate': 0.0008921043940918774, 'batch_size': 16, 'weight_decay': 7.547118515409804e-05}\n",
      "Mejores hiperparámetros guardados en 'best_params.json'.\n",
      "\n",
      "--- Iniciando Entrenamiento Final con Mejores Parámetros ---\n",
      "\n",
      "Iniciando entrenamiento por 100 épocas...\n",
      "Epoch 1/100, Train Loss: 1.080435, Val Loss: 1.019996\n",
      "---> Nuevo mejor modelo guardado en best_transformer_model.pth (Val Loss: 1.019996)\n",
      "---> Nuevo mejor modelo guardado en best_transformer_model.pth (Val Loss: 1.016055)\n",
      "Epoch 5/100, Train Loss: 1.004698, Val Loss: 1.016791\n",
      "Epoch 10/100, Train Loss: 1.001117, Val Loss: 1.015950\n",
      "---> Nuevo mejor modelo guardado en best_transformer_model.pth (Val Loss: 1.015950)\n",
      "---> Nuevo mejor modelo guardado en best_transformer_model.pth (Val Loss: 1.015801)\n",
      "Epoch 15/100, Train Loss: 0.999496, Val Loss: 1.016176\n",
      "Epoch 20/100, Train Loss: 0.998318, Val Loss: 1.015967\n",
      "---> Nuevo mejor modelo guardado en best_transformer_model.pth (Val Loss: 1.015686)\n",
      "Epoch 25/100, Train Loss: 0.997430, Val Loss: 1.016440\n",
      "Epoch 30/100, Train Loss: 0.997604, Val Loss: 1.016165\n",
      "Early stopping en época 32 después de 10 épocas sin mejora.\n",
      "Entrenamiento completado. Mejor Val Loss: 1.015686\n",
      "\n",
      "--- Entrenamiento Finalizado ---\n",
      "El mejor modelo basado en la validación se ha guardado como 'best_transformer_model.pth'.\n",
      "Los scalers están en 'scaler_input.pkl' y 'scaler_target.pkl'.\n",
      "Los mejores hiperparámetros están en 'best_params.json'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import math\n",
    "import pickle # Para guardar los scalers\n",
    "import json   # Para guardar los best_params\n",
    "\n",
    "# --- 1. Carga y Preprocesamiento de Datos ---\n",
    "print(\"Cargando y preprocesando datos...\")\n",
    "\n",
    "# [!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!]\n",
    "# [! Reemplaza esta sección con TU código para cargar          !]\n",
    "# [! input_tensor y target_tensor desde tus archivos .csv etc. !]\n",
    "# [! Ejemplo con datos aleatorios:                             !]\n",
    "num_scenarios = 500\n",
    "seq_len = 74 # Número de pares de relés por escenario\n",
    "input_features = 6\n",
    "output_features = 4\n",
    "input_tensor = torch.randn(num_scenarios, seq_len, input_features)\n",
    "target_tensor = torch.randn(num_scenarios, seq_len, output_features)\n",
    "print(f\"Datos de ejemplo generados: input_shape={input_tensor.shape}, target_shape={target_tensor.shape}\")\n",
    "# [!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!]\n",
    "\n",
    "\n",
    "# Normalización de los datos\n",
    "input_np = input_tensor.numpy().reshape(-1, input_tensor.shape[-1])\n",
    "target_np = target_tensor.numpy().reshape(-1, target_tensor.shape[-1])\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_target = StandardScaler()\n",
    "\n",
    "# Ajustar y transformar los scalers\n",
    "input_normalized = scaler_input.fit_transform(input_np)\n",
    "target_normalized = scaler_target.fit_transform(target_np)\n",
    "\n",
    "# Guardar los scalers para usarlos en la evaluación\n",
    "with open('scaler_input.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_input, f)\n",
    "with open('scaler_target.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_target, f)\n",
    "print(\"Scalers guardados como 'scaler_input.pkl' y 'scaler_target.pkl'.\")\n",
    "\n",
    "# Volver a convertir a tensores y restaurar la forma original\n",
    "input_tensor_normalized = torch.tensor(input_normalized, dtype=torch.float32).reshape(input_tensor.shape)\n",
    "target_tensor_normalized = torch.tensor(target_normalized, dtype=torch.float32).reshape(target_tensor.shape)\n",
    "\n",
    "# Dividir los datos en 80/20 (entrenamiento/validación) - Usar random_state fijo\n",
    "train_idx, val_idx = train_test_split(range(input_tensor_normalized.shape[0]), test_size=0.2, random_state=42)\n",
    "\n",
    "train_input = input_tensor_normalized[train_idx]\n",
    "train_target = target_tensor_normalized[train_idx]\n",
    "val_input = input_tensor_normalized[val_idx]\n",
    "val_target = target_tensor_normalized[val_idx]\n",
    "\n",
    "# Crear Datasets\n",
    "train_dataset = TensorDataset(train_input, train_target)\n",
    "val_dataset = TensorDataset(val_input, val_target)\n",
    "\n",
    "print(f\"Datos divididos: {len(train_dataset)} entrenamiento, {len(val_dataset)} validación.\")\n",
    "\n",
    "# --- 2. Definición del Modelo Transformer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # ... (código de PositionalEncoding sin cambios)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape expected: (seq_len, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False # TransformerEncoderLayer espera (seq_len, batch, feature)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len, input_dim)\n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model) # Escalar según paper \"Attention is All You Need\"\n",
    "        src = src.permute(1, 0, 2)  # Cambiar a (seq_len, batch_size, d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.permute(1, 0, 2)  # Volver a (batch_size, seq_len, d_model)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "# --- 3. Función de Entrenamiento ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_save_path='best_transformer_model.pth'):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    patience = 10 # Número de épocas para esperar antes de parar si no hay mejora\n",
    "\n",
    "    print(f\"\\nIniciando entrenamiento por {num_epochs} épocas...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_input, batch_target in train_loader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_input)\n",
    "            loss = criterion(output, batch_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_target in val_loader:\n",
    "                batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "                output = model(batch_input)\n",
    "                loss = criterion(output, batch_target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0: # Imprimir cada 5 épocas y la primera\n",
    "             print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')\n",
    "\n",
    "        # Guardar el mejor modelo y Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'---> Nuevo mejor modelo guardado en {model_save_path} (Val Loss: {best_val_loss:.6f})')\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping en época {epoch+1} después de {patience} épocas sin mejora.')\n",
    "                break # Detener el entrenamiento\n",
    "\n",
    "    print(f\"Entrenamiento completado. Mejor Val Loss: {best_val_loss:.6f}\")\n",
    "    return best_val_loss\n",
    "\n",
    "# --- 4. Optimización de Hiperparámetros con Optuna ---\n",
    "def objective(trial):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Hiperparámetros a optimizar\n",
    "    d_model = trial.suggest_categorical('d_model', [32, 64, 128]) # Reducido para pruebas rápidas\n",
    "    # Asegurar que nhead divida d_model\n",
    "    possible_nheads = [h for h in [2, 4, 8] if d_model % h == 0]\n",
    "    if not possible_nheads: # Si d_model es p.ej. 32, 8 no es posible\n",
    "        nhead = 2 # O el divisor más pequeño posible > 1\n",
    "    else:\n",
    "        nhead = trial.suggest_categorical('nhead', possible_nheads)\n",
    "\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 1, 4) # Reducido\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [128, 256, 512])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.4) # Rango ligeramente ajustado\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32]) # Reducido\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "\n",
    "    # Crear DataLoader con el batch_size sugerido\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        input_dim=train_input.shape[-1],\n",
    "        output_dim=train_target.shape[-1],\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Entrenar el modelo (usar menos épocas para la búsqueda)\n",
    "    num_optuna_epochs = 25 # Menos épocas para acelerar Optuna\n",
    "    model_save_path_trial = f'temp_best_model_trial_{trial.number}.pth' # Temporal\n",
    "    best_val_loss = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_optuna_epochs, device=device, model_save_path=model_save_path_trial)\n",
    "\n",
    "    # Limpiar archivo temporal (opcional)\n",
    "    import os\n",
    "    if os.path.exists(model_save_path_trial):\n",
    "        os.remove(model_save_path_trial)\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Ejecutar la optimización de hiperparámetros\n",
    "print(\"\\n--- Iniciando Optimización de Hiperparámetros (Optuna) ---\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# Aumenta n_trials para una búsqueda más exhaustiva\n",
    "study.optimize(objective, n_trials=15) # Número de pruebas de Optuna (ajusta según tu tiempo)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "print(\"\\n--- Mejores Hiperparámetros Encontrados ---\")\n",
    "print(best_params)\n",
    "\n",
    "# Guardar los mejores parámetros en un archivo JSON\n",
    "with open('best_params.json', 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "print(\"Mejores hiperparámetros guardados en 'best_params.json'.\")\n",
    "\n",
    "# --- 5. Entrenamiento Final con los Mejores Hiperparámetros ---\n",
    "print(\"\\n--- Iniciando Entrenamiento Final con Mejores Parámetros ---\")\n",
    "\n",
    "# Ajustar nhead por si acaso (Optuna podría devolver uno que no fue elegido explícitamente si no se usó suggest_categorical con la lista filtrada)\n",
    "d_model = best_params['d_model']\n",
    "possible_nheads = [h for h in [2, 4, 8] if d_model % h == 0]\n",
    "if best_params['nhead'] not in possible_nheads:\n",
    "     nhead = min(possible_nheads) if possible_nheads else 1 # O maneja error\n",
    "     print(f\"Ajustando nhead a {nhead} ya que {best_params['nhead']} no divide {d_model}\")\n",
    "else:\n",
    "    nhead = best_params['nhead']\n",
    "\n",
    "\n",
    "# Crear DataLoaders para el entrenamiento final\n",
    "final_batch_size = best_params['batch_size']\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=final_batch_size, shuffle=True)\n",
    "final_val_loader = DataLoader(val_dataset, batch_size=final_batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "final_model = TransformerModel(\n",
    "    input_dim=train_input.shape[-1],\n",
    "    output_dim=train_target.shape[-1],\n",
    "    d_model=best_params['d_model'],\n",
    "    nhead=nhead, # Usar el nhead ajustado\n",
    "    num_encoder_layers=best_params['num_encoder_layers'],\n",
    "    dim_feedforward=best_params['dim_feedforward'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "\n",
    "# Entrenar el modelo final (usar más épocas)\n",
    "final_num_epochs = 100 # O más, según sea necesario\n",
    "train_model(final_model, final_train_loader, final_val_loader, criterion, optimizer, num_epochs=final_num_epochs, device=device, model_save_path='best_transformer_model.pth')\n",
    "\n",
    "print(\"\\n--- Entrenamiento Finalizado ---\")\n",
    "print(\"El mejor modelo basado en la validación se ha guardado como 'best_transformer_model.pth'.\")\n",
    "print(\"Los scalers están en 'scaler_input.pkl' y 'scaler_target.pkl'.\")\n",
    "print(\"Los mejores hiperparámetros están en 'best_params.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando artefactos y datos para evaluación...\n",
      "Scalers cargados.\n",
      "Mejores hiperparámetros cargados: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 2, 'dim_feedforward': 256, 'dropout': 0.3363087097284188, 'learning_rate': 0.0008921043940918774, 'batch_size': 16, 'weight_decay': 7.547118515409804e-05}\n",
      "Datos originales de ejemplo cargados.\n",
      "Datos de validación preparados. Tamaño: torch.Size([100, 74, 6])\n",
      "Modelo cargado exitosamente desde 'best_transformer_model.pth' y puesto en modo evaluación.\n",
      "\n",
      "--- Evaluando Modelo en Datos de Validación ---\n",
      "\n",
      "Estadísticas del modelo en los datos de validación:\n",
      "Pérdida promedio (MSE normalizado): 1.005792\n",
      "Error Cuadrático Medio (MSE desnormalizado): 1.009080\n",
      "Error Absoluto Medio (MAE desnormalizado): 0.802382\n",
      "Coeficiente de Determinación (R² desnormalizado): -0.0002\n",
      "\n",
      "--- Prueba con un Ejemplo Específico (Primer Escenario de Validación) ---\n",
      "\n",
      "Predicciones vs Valores Reales (Desnormalizados) para los primeros 5 pares de relés:\n",
      "Par 1:\n",
      "  Predicción: [ 0.0152 -0.0227 -0.0097  0.0103]\n",
      "  Valor real: [-0.6274 -0.2780  0.6189 -0.7373]\n",
      "Par 2:\n",
      "  Predicción: [ 0.0115 -0.0032 -0.0020 -0.0251]\n",
      "  Valor real: [-0.4757 -0.9662  1.3379  1.8062]\n",
      "Par 3:\n",
      "  Predicción: [-0.0260 -0.0128  0.0271 -0.0117]\n",
      "  Valor real: [-1.5645 -0.4331  1.6746 -1.1328]\n",
      "Par 4:\n",
      "  Predicción: [ 0.0579 -0.0193 -0.0085 -0.0048]\n",
      "  Valor real: [ 0.5599 -0.4689  0.7011  2.0433]\n",
      "Par 5:\n",
      "  Predicción: [-0.0183 -0.0196 -0.0025  0.0011]\n",
      "  Valor real: [-0.6826  0.6235  0.2168  0.7623]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split # Para obtener el mismo split\n",
    "from sklearn.preprocessing import StandardScaler # Necesario para cargar scalers\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import pickle # Para cargar los scalers\n",
    "import json   # Para cargar los best_params\n",
    "import os\n",
    "\n",
    "# --- 1. Carga de Artefactos y Datos ---\n",
    "print(\"Cargando artefactos y datos para evaluación...\")\n",
    "\n",
    "# Nombres de los archivos guardados por el script de entrenamiento\n",
    "model_path = 'best_transformer_model.pth'\n",
    "scaler_input_path = 'scaler_input.pkl'\n",
    "scaler_target_path = 'scaler_target.pkl'\n",
    "params_path = 'best_params.json'\n",
    "\n",
    "# Verificar si los archivos existen\n",
    "if not all(os.path.exists(p) for p in [model_path, scaler_input_path, scaler_target_path, params_path]):\n",
    "    print(\"Error: Faltan archivos necesarios ('best_transformer_model.pth', '.pkl', 'best_params.json').\")\n",
    "    print(\"Asegúrate de haber ejecutado 'train_transformer.py' primero.\")\n",
    "    exit()\n",
    "\n",
    "# Cargar los scalers\n",
    "try:\n",
    "    with open(scaler_input_path, 'rb') as f:\n",
    "        scaler_input = pickle.load(f)\n",
    "    with open(scaler_target_path, 'rb') as f:\n",
    "        scaler_target = pickle.load(f)\n",
    "    print(\"Scalers cargados.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los scalers: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Cargar los mejores hiperparámetros\n",
    "try:\n",
    "    with open(params_path, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    print(\"Mejores hiperparámetros cargados:\", best_params)\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar 'best_params.json': {e}\")\n",
    "    exit()\n",
    "\n",
    "# [!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!]\n",
    "# [! Reemplaza esta sección con TU código para cargar los      !]\n",
    "# [! MISMOS datos ORIGINALES (input_tensor, target_tensor)    !]\n",
    "# [! que se usaron para entrenar.                             !]\n",
    "# [! Ejemplo con datos aleatorios (DEBEN SER LOS MISMOS):     !]\n",
    "num_scenarios = 500\n",
    "seq_len = 74\n",
    "input_features = 6\n",
    "output_features = 4\n",
    "# Es crucial generar/cargar los mismos datos que en el entrenamiento\n",
    "# Si usas aleatorios, fija la semilla: torch.manual_seed(42); np.random.seed(42) antes de generar\n",
    "input_tensor = torch.randn(num_scenarios, seq_len, input_features)\n",
    "target_tensor = torch.randn(num_scenarios, seq_len, output_features)\n",
    "print(f\"Datos originales de ejemplo cargados.\")\n",
    "# [!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!]\n",
    "\n",
    "\n",
    "# Normalizar los datos usando los SCALERS CARGADOS (transform, NO fit_transform)\n",
    "input_np = input_tensor.numpy().reshape(-1, input_tensor.shape[-1])\n",
    "target_np = target_tensor.numpy().reshape(-1, target_tensor.shape[-1])\n",
    "\n",
    "try:\n",
    "    input_normalized = scaler_input.transform(input_np)\n",
    "    target_normalized = scaler_target.transform(target_np)\n",
    "except Exception as e:\n",
    "    print(f\"Error al aplicar la transformación con los scalers cargados: {e}\")\n",
    "    print(\"Asegúrate de que los datos cargados tengan la forma correcta.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "input_tensor_normalized = torch.tensor(input_normalized, dtype=torch.float32).reshape(input_tensor.shape)\n",
    "target_tensor_normalized = torch.tensor(target_normalized, dtype=torch.float32).reshape(target_tensor.shape)\n",
    "\n",
    "# Obtener el MISMO split de validación usando el mismo random_state\n",
    "_, val_idx = train_test_split(range(input_tensor_normalized.shape[0]), test_size=0.2, random_state=42)\n",
    "val_input = input_tensor_normalized[val_idx]\n",
    "val_target = target_tensor_normalized[val_idx] # Datos normalizados\n",
    "val_target_original_shape = target_tensor[val_idx] # Datos originales para comparación final si es necesario\n",
    "\n",
    "print(f\"Datos de validación preparados. Tamaño: {val_input.shape}\")\n",
    "\n",
    "# Crear DataLoader para validación\n",
    "eval_batch_size = best_params.get('batch_size', 32) # Usar el batch_size de params o default\n",
    "val_dataset = TensorDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size=eval_batch_size)\n",
    "\n",
    "\n",
    "# --- 2. Definición del Modelo (Debe ser idéntica a la de entrenamiento) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # ... (Copia exacta de la clase PositionalEncoding de train_transformer.py)\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    # ... (Copia exacta de la clase TransformerModel de train_transformer.py)\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "# --- 3. Carga del Modelo Entrenado ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Extraer dimensiones y parámetros necesarios del entorno o datos cargados\n",
    "input_dim = val_input.shape[-1]\n",
    "output_dim = val_target.shape[-1]\n",
    "\n",
    "# Ajustar nhead si es necesario (basado en d_model de los parámetros cargados)\n",
    "d_model = best_params['d_model']\n",
    "nhead = best_params['nhead']\n",
    "possible_nheads = [h for h in [2, 4, 8] if d_model % h == 0]\n",
    "if nhead not in possible_nheads:\n",
    "     original_nhead = nhead\n",
    "     nhead = min(possible_nheads) if possible_nheads else 1\n",
    "     print(f\"Advertencia: nhead={original_nhead} en params no divide d_model={d_model}. Usando nhead={nhead} para instanciar.\")\n",
    "\n",
    "# Crear una instancia del modelo con los parámetros correctos\n",
    "model = TransformerModel(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    d_model=best_params['d_model'],\n",
    "    nhead=nhead, # Usar el nhead ajustado/verificado\n",
    "    num_encoder_layers=best_params['num_encoder_layers'],\n",
    "    dim_feedforward=best_params['dim_feedforward'],\n",
    "    dropout=best_params['dropout'] # El dropout durante eval no importa si se usa model.eval()\n",
    ").to(device)\n",
    "\n",
    "# Cargar los pesos guardados\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval() # ¡Muy importante! Poner el modelo en modo evaluación\n",
    "    print(f\"Modelo cargado exitosamente desde '{model_path}' y puesto en modo evaluación.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el state_dict del modelo: {e}\")\n",
    "    print(\"Verifica que los hiperparámetros en 'best_params.json' coincidan con los del modelo guardado.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 4. Funciones de Evaluación ---\n",
    "def evaluate_model(model, data_loader, criterion, scaler_target, device):\n",
    "    model.eval()\n",
    "    all_preds_normalized = []\n",
    "    all_targets_normalized = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_target in data_loader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            output = model(batch_input) # Predicciones normalizadas\n",
    "            loss = criterion(output, batch_target) # Pérdida sobre datos normalizados\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_preds_normalized.append(output.cpu().numpy())\n",
    "            all_targets_normalized.append(batch_target.cpu().numpy())\n",
    "\n",
    "    # Concatenar todas las predicciones y valores reales (normalizados)\n",
    "    all_preds_normalized = np.concatenate(all_preds_normalized, axis=0)\n",
    "    all_targets_normalized = np.concatenate(all_targets_normalized, axis=0)\n",
    "\n",
    "    # Desnormalizar (cuidado con la forma)\n",
    "    num_samples = all_preds_normalized.shape[0]\n",
    "    seq_len = all_preds_normalized.shape[1]\n",
    "    output_features = all_preds_normalized.shape[2]\n",
    "\n",
    "    preds_flat = all_preds_normalized.reshape(-1, output_features)\n",
    "    targets_flat = all_targets_normalized.reshape(-1, output_features)\n",
    "\n",
    "    preds_denorm_flat = scaler_target.inverse_transform(preds_flat)\n",
    "    targets_denorm_flat = scaler_target.inverse_transform(targets_flat)\n",
    "\n",
    "    # Restaurar forma original (si es necesario, aunque para métricas no suele serlo)\n",
    "    all_preds_denorm = preds_denorm_flat.reshape(num_samples, seq_len, output_features)\n",
    "    all_targets_denorm = targets_denorm_flat.reshape(num_samples, seq_len, output_features)\n",
    "\n",
    "\n",
    "    # Calcular métricas sobre los datos desnormalizados\n",
    "    # Usar las versiones aplanadas para las métricas sklearn estándar\n",
    "    mse = mean_squared_error(targets_denorm_flat, preds_denorm_flat)\n",
    "    mae = mean_absolute_error(targets_denorm_flat, preds_denorm_flat)\n",
    "    r2 = r2_score(targets_denorm_flat, preds_denorm_flat)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader) # Pérdida promedio (normalizada)\n",
    "    return avg_loss, mse, mae, r2, all_preds_denorm, all_targets_denorm\n",
    "\n",
    "def test_model(model, input_data_normalized, scaler_target, device):\n",
    "    # input_data_normalized debe ser un solo escenario (seq_len, input_dim) o (1, seq_len, input_dim)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(input_data_normalized, dtype=torch.float32).to(device)\n",
    "        # Asegurarse que tenga la dimensión de batch\n",
    "        if len(input_tensor.shape) == 2:\n",
    "            input_tensor = input_tensor.unsqueeze(0) # Añadir dim de batch: (1, seq_len, input_dim)\n",
    "\n",
    "        output_normalized = model(input_tensor) # (1, seq_len, output_dim)\n",
    "\n",
    "        # Desnormalizar\n",
    "        output_np_norm = output_normalized.cpu().numpy()\n",
    "        output_features = output_np_norm.shape[-1]\n",
    "        output_flat_norm = output_np_norm.reshape(-1, output_features)\n",
    "        output_flat_denorm = scaler_target.inverse_transform(output_flat_norm)\n",
    "        output_denorm = output_flat_denorm.reshape(output_np_norm.shape) # (1, seq_len, output_dim)\n",
    "\n",
    "        return output_denorm.squeeze(0) # Quitar la dimension de batch para devolver (seq_len, output_dim)\n",
    "\n",
    "\n",
    "# --- 5. Ejecución de la Evaluación ---\n",
    "criterion = nn.MSELoss() # Se usa para calcular la pérdida normalizada\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación completo\n",
    "print(\"\\n--- Evaluando Modelo en Datos de Validación ---\")\n",
    "avg_loss, mse, mae, r2, predictions_denorm, true_values_denorm = evaluate_model(model, val_loader, criterion, scaler_target, device)\n",
    "\n",
    "print(\"\\nEstadísticas del modelo en los datos de validación:\")\n",
    "print(f\"Pérdida promedio (MSE normalizado): {avg_loss:.6f}\")\n",
    "print(f\"Error Cuadrático Medio (MSE desnormalizado): {mse:.6f}\")\n",
    "print(f\"Error Absoluto Medio (MAE desnormalizado): {mae:.6f}\")\n",
    "print(f\"Coeficiente de Determinación (R² desnormalizado): {r2:.4f}\")\n",
    "\n",
    "# Probar el modelo con un ejemplo específico (el primer escenario del conjunto de validación)\n",
    "print(\"\\n--- Prueba con un Ejemplo Específico (Primer Escenario de Validación) ---\")\n",
    "test_input_normalized = val_input[0].cpu().numpy() # (seq_len, input_dim)\n",
    "# El valor real correspondiente ya lo tenemos desnormalizado de evaluate_model\n",
    "true_output_denorm_sample = true_values_denorm[0] # (seq_len, output_dim)\n",
    "\n",
    "predicted_output_denorm_sample = test_model(model, test_input_normalized, scaler_target, device) # (seq_len, output_dim)\n",
    "\n",
    "# Mostrar los resultados de la prueba para los primeros N pares\n",
    "num_pairs_to_show = 5\n",
    "print(f\"\\nPredicciones vs Valores Reales (Desnormalizados) para los primeros {num_pairs_to_show} pares de relés:\")\n",
    "for i in range(min(seq_len, num_pairs_to_show)):\n",
    "    print(f\"Par {i+1}:\")\n",
    "    # Formatear para mejor legibilidad\n",
    "    pred_str = np.array2string(predicted_output_denorm_sample[i, :], precision=4, floatmode='fixed')\n",
    "    true_str = np.array2string(true_output_denorm_sample[i, :], precision=4, floatmode='fixed')\n",
    "    print(f\"  Predicción: {pred_str}\")\n",
    "    print(f\"  Valor real: {true_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:36:15,043] A new study created in memory with name: no-name-c5ecf6d3-a62a-4301-9336-5703d448fb12\n",
      "/var/folders/pz/w3bcsy5x2wgb_7c0fs6qmkp80000gn/T/ipykernel_63708/1349000200.py:130: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.3)\n",
      "/var/folders/pz/w3bcsy5x2wgb_7c0fs6qmkp80000gn/T/ipykernel_63708/1349000200.py:131: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "/var/folders/pz/w3bcsy5x2wgb_7c0fs6qmkp80000gn/T/ipykernel_63708/1349000200.py:133: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)  # Regularización L2\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 1.0660, Val Loss: 0.9674\n",
      "Epoch 20/50, Train Loss: 1.0319, Val Loss: 0.9426\n",
      "Epoch 30/50, Train Loss: 1.0204, Val Loss: 0.9453\n",
      "Epoch 40/50, Train Loss: 1.0031, Val Loss: 0.9459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:36:40,886] Trial 0 finished with value: 0.9419369697570801 and parameters: {'d_model': 256, 'nhead': 8, 'num_encoder_layers': 5, 'dim_feedforward': 512, 'dropout': 0.21303799415208624, 'learning_rate': 0.0031011514758145188, 'batch_size': 32, 'weight_decay': 0.004120132999818286}. Best is trial 0 with value: 0.9419369697570801.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 1.0121, Val Loss: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/w3bcsy5x2wgb_7c0fs6qmkp80000gn/T/ipykernel_63708/1349000200.py:130: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.3)\n",
      "/var/folders/pz/w3bcsy5x2wgb_7c0fs6qmkp80000gn/T/ipykernel_63708/1349000200.py:131: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "/var/folders/pz/w3bcsy5x2wgb_7c0fs6qmkp80000gn/T/ipykernel_63708/1349000200.py:133: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)  # Regularización L2\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.3682, Val Loss: 0.3358\n",
      "Epoch 20/50, Train Loss: 0.1786, Val Loss: 0.1626\n",
      "Epoch 30/50, Train Loss: 0.0938, Val Loss: 0.0912\n",
      "Epoch 40/50, Train Loss: 0.0522, Val Loss: 0.0563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:36:44,469] Trial 1 finished with value: 0.04534054547548294 and parameters: {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 1, 'dim_feedforward': 128, 'dropout': 0.14643364570918843, 'learning_rate': 0.0018446960698927804, 'batch_size': 32, 'weight_decay': 7.526837086957923e-05}. Best is trial 1 with value: 0.04534054547548294.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0377, Val Loss: 0.0458\n",
      "Epoch 10/50, Train Loss: 0.3164, Val Loss: 0.2980\n",
      "Epoch 20/50, Train Loss: 0.1257, Val Loss: 0.1174\n",
      "Epoch 30/50, Train Loss: 0.0586, Val Loss: 0.0635\n",
      "Epoch 40/50, Train Loss: 0.0358, Val Loss: 0.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:36:51,194] Trial 2 finished with value: 0.03157302364706993 and parameters: {'d_model': 256, 'nhead': 2, 'num_encoder_layers': 1, 'dim_feedforward': 512, 'dropout': 0.14038590394434391, 'learning_rate': 0.003007830107786523, 'batch_size': 32, 'weight_decay': 2.2647870313852387e-05}. Best is trial 2 with value: 0.03157302364706993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0247, Val Loss: 0.0316\n",
      "Epoch 10/50, Train Loss: 0.6916, Val Loss: 0.6292\n",
      "Epoch 20/50, Train Loss: 0.4862, Val Loss: 0.4268\n",
      "Epoch 30/50, Train Loss: 0.2967, Val Loss: 0.2698\n",
      "Epoch 40/50, Train Loss: 0.1901, Val Loss: 0.1773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:36:57,998] Trial 3 finished with value: 0.11623222380876541 and parameters: {'d_model': 128, 'nhead': 8, 'num_encoder_layers': 2, 'dim_feedforward': 128, 'dropout': 0.18480547173463935, 'learning_rate': 0.00011621368108340085, 'batch_size': 16, 'weight_decay': 0.00015282243844425946}. Best is trial 2 with value: 0.03157302364706993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.1207, Val Loss: 0.1162\n",
      "Epoch 10/50, Train Loss: 0.7423, Val Loss: 0.6765\n",
      "Epoch 20/50, Train Loss: 0.5978, Val Loss: 0.5468\n",
      "Epoch 30/50, Train Loss: 0.4805, Val Loss: 0.4334\n",
      "Epoch 40/50, Train Loss: 0.3717, Val Loss: 0.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:37:04,929] Trial 4 finished with value: 0.26231876015663147 and parameters: {'d_model': 128, 'nhead': 2, 'num_encoder_layers': 3, 'dim_feedforward': 256, 'dropout': 0.2894294608481086, 'learning_rate': 0.0002610178660512947, 'batch_size': 64, 'weight_decay': 3.840700022235123e-05}. Best is trial 2 with value: 0.03157302364706993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.2829, Val Loss: 0.2623\n",
      "Epoch 10/50, Train Loss: 0.6474, Val Loss: 0.5907\n",
      "Epoch 20/50, Train Loss: 0.4771, Val Loss: 0.4367\n",
      "Epoch 30/50, Train Loss: 0.3233, Val Loss: 0.2975\n",
      "Epoch 40/50, Train Loss: 0.2183, Val Loss: 0.2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:37:10,844] Trial 5 finished with value: 0.13925901055335999 and parameters: {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 512, 'dropout': 0.106504064896535, 'learning_rate': 0.0001250628079709077, 'batch_size': 32, 'weight_decay': 6.827650275796048e-05}. Best is trial 2 with value: 0.03157302364706993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.1429, Val Loss: 0.1393\n",
      "Epoch 10/50, Train Loss: 1.0917, Val Loss: 1.0023\n",
      "Epoch 20/50, Train Loss: 0.9206, Val Loss: 0.8411\n",
      "Epoch 30/50, Train Loss: 0.8029, Val Loss: 0.7337\n",
      "Epoch 40/50, Train Loss: 0.7233, Val Loss: 0.6603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:37:13,155] Trial 6 finished with value: 0.6078706383705139 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 1, 'dim_feedforward': 512, 'dropout': 0.1541078948560042, 'learning_rate': 0.0001438887831733158, 'batch_size': 64, 'weight_decay': 4.581427110580153e-05}. Best is trial 2 with value: 0.03157302364706993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.6658, Val Loss: 0.6079\n",
      "Epoch 10/50, Train Loss: 0.7318, Val Loss: 0.6562\n",
      "Epoch 20/50, Train Loss: 0.5680, Val Loss: 0.5122\n",
      "Epoch 30/50, Train Loss: 0.4539, Val Loss: 0.4081\n",
      "Epoch 40/50, Train Loss: 0.3464, Val Loss: 0.3162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:37:20,917] Trial 7 finished with value: 0.2362021803855896 and parameters: {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 2, 'dim_feedforward': 512, 'dropout': 0.2523249189030542, 'learning_rate': 0.0002660607897954807, 'batch_size': 64, 'weight_decay': 0.001070223750301344}. Best is trial 2 with value: 0.03157302364706993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.2535, Val Loss: 0.2362\n",
      "Epoch 10/50, Train Loss: 0.6198, Val Loss: 0.6038\n",
      "Epoch 20/50, Train Loss: 0.3885, Val Loss: 0.3604\n",
      "Epoch 30/50, Train Loss: 0.2053, Val Loss: 0.1884\n",
      "Epoch 40/50, Train Loss: 0.0991, Val Loss: 0.0962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:37:51,742] Trial 8 finished with value: 0.06729038804769516 and parameters: {'d_model': 256, 'nhead': 8, 'num_encoder_layers': 6, 'dim_feedforward': 256, 'dropout': 0.28697797202075104, 'learning_rate': 3.866471449118007e-05, 'batch_size': 16, 'weight_decay': 0.00021766928742243395}. Best is trial 2 with value: 0.03157302364706993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0605, Val Loss: 0.0673\n",
      "Epoch 10/50, Train Loss: 0.1795, Val Loss: 0.1798\n",
      "Epoch 20/50, Train Loss: 0.0419, Val Loss: 0.0463\n",
      "Epoch 30/50, Train Loss: 0.0223, Val Loss: 0.0275\n",
      "Epoch 40/50, Train Loss: 0.0160, Val Loss: 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:37:59,679] Trial 9 finished with value: 0.02057831548154354 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.12701899235413902, 'learning_rate': 0.001758834453253742, 'batch_size': 16, 'weight_decay': 0.00012014700596852297}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0184, Val Loss: 0.0239\n",
      "Epoch 10/50, Train Loss: 0.2520, Val Loss: 0.2241\n",
      "Epoch 20/50, Train Loss: 0.0669, Val Loss: 0.0693\n",
      "Epoch 30/50, Train Loss: 0.0338, Val Loss: 0.0411\n",
      "Epoch 40/50, Train Loss: 0.0239, Val Loss: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:38:10,529] Trial 10 finished with value: 0.023838568478822708 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 128, 'dropout': 0.1013170208989892, 'learning_rate': 0.0011212853157921113, 'batch_size': 16, 'weight_decay': 2.842004631133699e-06}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0205, Val Loss: 0.0238\n",
      "Epoch 10/50, Train Loss: 0.2441, Val Loss: 0.2230\n",
      "Epoch 20/50, Train Loss: 0.0781, Val Loss: 0.0831\n",
      "Epoch 30/50, Train Loss: 0.0426, Val Loss: 0.0541\n",
      "Epoch 40/50, Train Loss: 0.0255, Val Loss: 0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:38:22,447] Trial 11 finished with value: 0.02368009276688099 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 128, 'dropout': 0.11146843022689354, 'learning_rate': 0.001108831384301685, 'batch_size': 16, 'weight_decay': 1.5825691315433117e-06}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0197, Val Loss: 0.0237\n",
      "Epoch 10/50, Train Loss: 0.8546, Val Loss: 0.7757\n",
      "Epoch 20/50, Train Loss: 0.7483, Val Loss: 0.6889\n",
      "Epoch 30/50, Train Loss: 0.5619, Val Loss: 0.5570\n",
      "Epoch 40/50, Train Loss: 0.6406, Val Loss: 0.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:38:32,330] Trial 12 finished with value: 0.4758501350879669 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 4, 'dim_feedforward': 128, 'dropout': 0.12608542993078775, 'learning_rate': 0.007678035041415385, 'batch_size': 16, 'weight_decay': 1.1070453059280337e-06}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.7542, Val Loss: 0.6907\n",
      "Epoch 10/50, Train Loss: 0.3441, Val Loss: 0.3138\n",
      "Epoch 20/50, Train Loss: 0.1084, Val Loss: 0.1053\n",
      "Epoch 30/50, Train Loss: 0.0647, Val Loss: 0.0681\n",
      "Epoch 40/50, Train Loss: 0.0338, Val Loss: 0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:38:39,280] Trial 13 finished with value: 0.029767794534564018 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 3, 'dim_feedforward': 128, 'dropout': 0.1757789676990953, 'learning_rate': 0.0010912597673119193, 'batch_size': 16, 'weight_decay': 1.0881779567341535e-05}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0258, Val Loss: 0.0298\n",
      "Epoch 10/50, Train Loss: 0.3719, Val Loss: 0.3278\n",
      "Epoch 20/50, Train Loss: 0.1136, Val Loss: 0.1049\n",
      "Epoch 30/50, Train Loss: 0.0465, Val Loss: 0.0498\n",
      "Epoch 40/50, Train Loss: 0.0372, Val Loss: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:38:52,022] Trial 14 finished with value: 0.02961997129023075 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 5, 'dim_feedforward': 256, 'dropout': 0.20677072728869045, 'learning_rate': 0.0006060229119104824, 'batch_size': 16, 'weight_decay': 0.0004747236039878941}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0246, Val Loss: 0.0296\n",
      "Epoch 10/50, Train Loss: 0.9347, Val Loss: 0.8650\n",
      "Epoch 20/50, Train Loss: 0.8920, Val Loss: 0.8628\n",
      "Epoch 30/50, Train Loss: 0.9316, Val Loss: 0.8628\n",
      "Epoch 40/50, Train Loss: 0.8871, Val Loss: 0.8614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:39:00,318] Trial 15 finished with value: 0.8613825440406799 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 4, 'dim_feedforward': 512, 'dropout': 0.11589192533065852, 'learning_rate': 0.009119157395653556, 'batch_size': 16, 'weight_decay': 5.504664792276691e-06}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.9131, Val Loss: 0.8624\n",
      "Epoch 10/50, Train Loss: 1.1663, Val Loss: 1.0593\n",
      "Epoch 20/50, Train Loss: 0.9973, Val Loss: 0.9634\n",
      "Epoch 30/50, Train Loss: 1.0262, Val Loss: 0.9032\n",
      "Epoch 40/50, Train Loss: 0.9224, Val Loss: 0.8572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:39:11,848] Trial 16 finished with value: 0.8169777393341064 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 5, 'dim_feedforward': 128, 'dropout': 0.1643526510709878, 'learning_rate': 1.3032560333199646e-05, 'batch_size': 16, 'weight_decay': 0.002327535959533399}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.8544, Val Loss: 0.8170\n",
      "Epoch 10/50, Train Loss: 0.3003, Val Loss: 0.2628\n",
      "Epoch 20/50, Train Loss: 0.0983, Val Loss: 0.0942\n",
      "Epoch 30/50, Train Loss: 0.0416, Val Loss: 0.0463\n",
      "Epoch 40/50, Train Loss: 0.0370, Val Loss: 0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:39:20,996] Trial 17 finished with value: 0.027140503749251366 and parameters: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.23348947715797252, 'learning_rate': 0.0006186079699422896, 'batch_size': 16, 'weight_decay': 0.00044995433825662033}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0233, Val Loss: 0.0291\n",
      "Epoch 10/50, Train Loss: 0.9577, Val Loss: 0.8419\n",
      "Epoch 20/50, Train Loss: 0.2530, Val Loss: 0.2182\n",
      "Epoch 30/50, Train Loss: 0.1963, Val Loss: 0.1575\n",
      "Epoch 40/50, Train Loss: 0.0611, Val Loss: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:39:34,205] Trial 18 finished with value: 0.06625315546989441 and parameters: {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 6, 'dim_feedforward': 128, 'dropout': 0.12866413018695816, 'learning_rate': 0.004780505085180394, 'batch_size': 16, 'weight_decay': 0.009828309799503299}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0600, Val Loss: 0.0664\n",
      "Epoch 10/50, Train Loss: 0.6757, Val Loss: 0.6061\n",
      "Epoch 20/50, Train Loss: 0.4657, Val Loss: 0.4257\n",
      "Epoch 30/50, Train Loss: 0.3195, Val Loss: 0.2934\n",
      "Epoch 40/50, Train Loss: 0.2252, Val Loss: 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 16:39:40,021] Trial 19 finished with value: 0.15645889937877655 and parameters: {'d_model': 64, 'nhead': 2, 'num_encoder_layers': 4, 'dim_feedforward': 256, 'dropout': 0.18676617546368462, 'learning_rate': 0.0007542397826772367, 'batch_size': 64, 'weight_decay': 1.084152986790639e-06}. Best is trial 9 with value: 0.02057831548154354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.1621, Val Loss: 0.1565\n",
      "Mejores hiperparámetros: {'d_model': 64, 'nhead': 8, 'num_encoder_layers': 3, 'dim_feedforward': 512, 'dropout': 0.12701899235413902, 'learning_rate': 0.001758834453253742, 'batch_size': 16, 'weight_decay': 0.00012014700596852297}\n",
      "Epoch 10/100, Train Loss: 0.2241, Val Loss: 0.1949\n",
      "Epoch 20/100, Train Loss: 0.0525, Val Loss: 0.0535\n",
      "Epoch 30/100, Train Loss: 0.0286, Val Loss: 0.0331\n",
      "Epoch 40/100, Train Loss: 0.0203, Val Loss: 0.0260\n",
      "Epoch 50/100, Train Loss: 0.0186, Val Loss: 0.0238\n",
      "Epoch 60/100, Train Loss: 0.0135, Val Loss: 0.0208\n",
      "Epoch 70/100, Train Loss: 0.0121, Val Loss: 0.0184\n",
      "Epoch 80/100, Train Loss: 0.0112, Val Loss: 0.0193\n",
      "Epoch 90/100, Train Loss: 0.0118, Val Loss: 0.0179\n",
      "Epoch 100/100, Train Loss: 0.0113, Val Loss: 0.0176\n",
      "Entrenamiento finalizado. Modelo guardado como 'best_transformer_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import math\n",
    "\n",
    "# [Código previo para cargar y procesar los datos]\n",
    "# ... (tu código para cargar input_tensor y target_tensor)\n",
    "\n",
    "# Normalización de los datos\n",
    "input_np = input_tensor.numpy().reshape(-1, input_tensor.shape[-1])  # Aplanar para normalizar\n",
    "target_np = target_tensor.numpy().reshape(-1, target_tensor.shape[-1])\n",
    "\n",
    "scaler_input = StandardScaler()\n",
    "scaler_target = StandardScaler()\n",
    "\n",
    "input_normalized = scaler_input.fit_transform(input_np)\n",
    "target_normalized = scaler_target.fit_transform(target_np)\n",
    "\n",
    "# Volver a convertir a tensores y restaurar la forma original\n",
    "input_tensor_normalized = torch.tensor(input_normalized, dtype=torch.float32).reshape(input_tensor.shape)\n",
    "target_tensor_normalized = torch.tensor(target_normalized, dtype=torch.float32).reshape(target_tensor.shape)\n",
    "\n",
    "# Dividir los datos en 80/20 (entrenamiento/validación)\n",
    "train_idx, val_idx = train_test_split(range(input_tensor_normalized.shape[0]), test_size=0.2, random_state=42)\n",
    "\n",
    "train_input = input_tensor_normalized[train_idx]\n",
    "train_target = target_tensor_normalized[train_idx]\n",
    "val_input = input_tensor_normalized[val_idx]\n",
    "val_target = target_tensor_normalized[val_idx]\n",
    "\n",
    "# Crear DataLoader para manejar los datos en lotes\n",
    "train_dataset = TensorDataset(train_input, train_target)\n",
    "val_dataset = TensorDataset(val_input, val_target)\n",
    "\n",
    "# Definir la arquitectura del Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Proyección lineal para ajustar la dimensión de entrada a d_model\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Capas del Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Proyección lineal para la salida\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len, input_dim)\n",
    "        src = self.input_proj(src)  # Proyectar a d_model\n",
    "        src = src.permute(1, 0, 2)  # Cambiar a (seq_len, batch_size, d_model) para Transformer\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.permute(1, 0, 2)  # Volver a (batch_size, seq_len, d_model)\n",
    "        output = self.output_proj(output)  # Proyectar a output_dim\n",
    "        return output\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_input, batch_target in train_loader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_input)\n",
    "            loss = criterion(output, batch_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_target in val_loader:\n",
    "                batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "                output = model(batch_input)\n",
    "                loss = criterion(output, batch_target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "    return best_val_loss\n",
    "\n",
    "# Ajuste de hiperparámetros con Optuna\n",
    "def objective(trial):\n",
    "    # Hiperparámetros a optimizar\n",
    "    d_model = trial.suggest_categorical('d_model', [64, 128, 256])\n",
    "    nhead = trial.suggest_categorical('nhead', [2, 4, 8])  # Debe dividir d_model\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 1, 6)\n",
    "    dim_feedforward = trial.suggest_categorical('dim_feedforward', [128, 256, 512])\n",
    "    dropout = trial.suggest_uniform('dropout', 0.1, 0.3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)  # Regularización L2\n",
    "\n",
    "    # Asegurar que nhead divida d_model\n",
    "    if d_model % nhead != 0:\n",
    "        nhead = min([h for h in [2, 4, 8] if d_model % h == 0], default=2)\n",
    "\n",
    "    # Crear DataLoader con el batch_size sugerido\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerModel(\n",
    "        input_dim=train_input.shape[-1],  # 6\n",
    "        output_dim=train_target.shape[-1],  # 4\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    best_val_loss = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, device=device)\n",
    "    return best_val_loss\n",
    "\n",
    "# Ejecutar la optimización de hiperparámetros\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Ajustar nhead para que divida d_model\n",
    "d_model = best_params['d_model']\n",
    "nhead = best_params['nhead']\n",
    "if d_model % nhead != 0:\n",
    "    nhead = min([h for h in [2, 4, 8] if d_model % h == 0], default=2)\n",
    "\n",
    "# Entrenar el modelo final con los mejores hiperparámetros\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "final_model = TransformerModel(\n",
    "    input_dim=train_input.shape[-1],\n",
    "    output_dim=train_target.shape[-1],\n",
    "    d_model=best_params['d_model'],\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=best_params['num_encoder_layers'],\n",
    "    dim_feedforward=best_params['dim_feedforward'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "\n",
    "# Entrenar el modelo final\n",
    "train_model(final_model, train_loader, val_loader, criterion, optimizer, num_epochs=100, device=device)\n",
    "\n",
    "# Cargar el mejor modelo guardado\n",
    "final_model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
    "print(\"Entrenamiento finalizado. Modelo guardado como 'best_transformer_model.pth'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TransformerModel:\n\tsize mismatch for input_proj.weight: copying a param with shape torch.Size([64, 6]) from checkpoint, the shape in current model is torch.Size([128, 6]).\n\tsize mismatch for input_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pos_encoder.pe: copying a param with shape torch.Size([5000, 1, 64]) from checkpoint, the shape in current model is torch.Size([5000, 1, 128]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for output_proj.weight: copying a param with shape torch.Size([4, 64]) from checkpoint, the shape in current model is torch.Size([4, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Cargar el modelo\u001b[39;00m\n\u001b[1;32m    141\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_transformer_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 142\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_encoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Crear DataLoader para los datos de validación (o prueba)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Asume que tienes val_input y val_target del código anterior\u001b[39;00m\n\u001b[1;32m    146\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(val_input, val_target)\n",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_path, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, device):\n\u001b[1;32m     60\u001b[0m     model \u001b[38;5;241m=\u001b[39m TransformerModel(\n\u001b[1;32m     61\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[1;32m     62\u001b[0m         output_dim\u001b[38;5;241m=\u001b[39moutput_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mdropout\n\u001b[1;32m     68\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransformerModel:\n\tsize mismatch for input_proj.weight: copying a param with shape torch.Size([64, 6]) from checkpoint, the shape in current model is torch.Size([128, 6]).\n\tsize mismatch for input_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pos_encoder.pe: copying a param with shape torch.Size([5000, 1, 64]) from checkpoint, the shape in current model is torch.Size([5000, 1, 128]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for output_proj.weight: copying a param with shape torch.Size([4, 64]) from checkpoint, the shape in current model is torch.Size([4, 128])."
     ]
    }
   ],
   "source": [
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import numpy as np\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    import math\n",
    "\n",
    "    # Definir la clase PositionalEncoding (necesaria para el Transformer)\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, max_len=5000):\n",
    "            super(PositionalEncoding, self).__init__()\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:x.size(0), :]\n",
    "            return x\n",
    "\n",
    "    # Definir la clase del modelo Transformer (necesaria para cargar el modelo)\n",
    "    class TransformerModel(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "            super(TransformerModel, self).__init__()\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            self.d_model = d_model\n",
    "\n",
    "            # Proyección lineal para ajustar la dimensión de entrada a d_model\n",
    "            self.input_proj = nn.Linear(input_dim, d_model)\n",
    "            self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "            # Capas del Transformer\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "            # Proyección lineal para la salida\n",
    "            self.output_proj = nn.Linear(d_model, output_dim)\n",
    "\n",
    "        def forward(self, src):\n",
    "            # src shape: (batch_size, seq_len, input_dim)\n",
    "            src = self.input_proj(src)  # Proyectar a d_model\n",
    "            src = src.permute(1, 0, 2)  # Cambiar a (seq_len, batch_size, d_model) para Transformer\n",
    "            src = self.pos_encoder(src)\n",
    "            output = self.transformer_encoder(src)\n",
    "            output = output.permute(1, 0, 2)  # Volver a (batch_size, seq_len, d_model)\n",
    "            output = self.output_proj(output)  # Proyectar a output_dim\n",
    "            return output\n",
    "\n",
    "    # Cargar el modelo guardado\n",
    "    def load_model(model_path, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, device):\n",
    "        model = TransformerModel(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    # Función para evaluar el modelo y calcular métricas\n",
    "    def evaluate_model(model, data_loader, criterion, scaler_target, device):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_target in data_loader:\n",
    "                batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "                output = model(batch_input)\n",
    "                loss = criterion(output, batch_target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Guardar predicciones y valores reales\n",
    "                all_preds.append(output.cpu().numpy())\n",
    "                all_targets.append(batch_target.cpu().numpy())\n",
    "\n",
    "        # Concatenar todas las predicciones y valores reales\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "        # Desnormalizar las predicciones y los valores reales\n",
    "        all_preds_denorm = scaler_target.inverse_transform(all_preds.reshape(-1, all_preds.shape[-1])).reshape(all_preds.shape)\n",
    "        all_targets_denorm = scaler_target.inverse_transform(all_targets.reshape(-1, all_targets.shape[-1])).reshape(all_targets.shape)\n",
    "\n",
    "        # Calcular métricas en la escala original\n",
    "        mse = mean_squared_error(all_targets_denorm.reshape(-1), all_preds_denorm.reshape(-1))\n",
    "        mae = mean_absolute_error(all_targets_denorm.reshape(-1), all_preds_denorm.reshape(-1))\n",
    "        r2 = r2_score(all_targets_denorm.reshape(-1), all_preds_denorm.reshape(-1))\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        return avg_loss, mse, mae, r2, all_preds_denorm, all_targets_denorm\n",
    "\n",
    "    # Función para probar el modelo con un ejemplo específico\n",
    "    def test_model(model, input_data, scaler_input, scaler_target, device):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Normalizar el dato de entrada\n",
    "            input_np = input_data.numpy().reshape(-1, input_data.shape[-1])\n",
    "            input_normalized = scaler_input.transform(input_np).reshape(input_data.shape)\n",
    "            input_tensor = torch.tensor(input_normalized, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Hacer la predicción\n",
    "            if len(input_tensor.shape) == 2:  # Si es un solo ejemplo, agregar dimensión de batch\n",
    "                input_tensor = input_tensor.unsqueeze(0)\n",
    "            output = model(input_tensor)\n",
    "\n",
    "            # Desnormalizar la predicción\n",
    "            output_np = output.cpu().numpy()\n",
    "            output_denorm = scaler_target.inverse_transform(output_np.reshape(-1, output_np.shape[-1])).reshape(output_np.shape)\n",
    "            return output_denorm\n",
    "\n",
    "    # Configuración\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Hiperparámetros del modelo (AJUSTA ESTOS VALORES SEGÚN LOS MEJORES HIPERPARÁMETROS DE OPTUNA)\n",
    "    # Reemplaza estos valores con los que obtuviste al entrenar el modelo\n",
    "    d_model = 128              # Ajusta según best_params['d_model']\n",
    "    nhead = 4                  # Ajusta según best_params['nhead']\n",
    "    num_encoder_layers = 3     # Ajusta según best_params['num_encoder_layers']\n",
    "    dim_feedforward = 512      # Ajusta según best_params['dim_feedforward'] (esto causaba el error, era 512 en el modelo guardado)\n",
    "    dropout = 0.1              # Ajusta según best_params['dropout']\n",
    "    input_dim = 6              # Número de características de entrada por par de relés\n",
    "    output_dim = 4             # Número de características de salida por par de relés\n",
    "    batch_size = 32            # Ajusta según best_params['batch_size']\n",
    "\n",
    "    # Cargar el modelo\n",
    "    model_path = 'best_transformer_model.pth'\n",
    "    model = load_model(model_path, input_dim, output_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, device)\n",
    "\n",
    "    # Crear DataLoader para los datos de validación (o prueba)\n",
    "    # Asume que tienes val_input y val_target del código anterior\n",
    "    val_dataset = TensorDataset(val_input, val_target)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Definir el criterio de pérdida\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Evaluar el modelo en los datos de validación\n",
    "    avg_loss, mse, mae, r2, predictions, true_values = evaluate_model(model, val_loader, criterion, scaler_target, device)\n",
    "\n",
    "    # Mostrar estadísticas\n",
    "    print(\"Estadísticas del modelo en los datos de validación:\")\n",
    "    print(f\"Pérdida promedio (MSE normalizado): {avg_loss:.4f}\")\n",
    "    print(f\"Error Cuadrático Medio (MSE desnormalizado): {mse:.4f}\")\n",
    "    print(f\"Error Absoluto Medio (MAE desnormalizado): {mae:.4f}\")\n",
    "    print(f\"Coeficiente de Determinación (R² desnormalizado): {r2:.4f}\")\n",
    "\n",
    "    # Probar el modelo con un ejemplo específico (por ejemplo, el primer escenario de validación)\n",
    "    test_input = val_input[0]  # Primer escenario de validación\n",
    "    predicted_output = test_model(model, test_input, scaler_input, scaler_target, device)\n",
    "    true_output = scaler_target.inverse_transform(val_target[0].numpy().reshape(-1, val_target.shape[-1])).reshape(val_target[0].shape)\n",
    "\n",
    "    # Mostrar los resultados de la prueba\n",
    "    print(\"\\nPrueba con un ejemplo específico (primer escenario de validación):\")\n",
    "    print(\"Predicciones (desnormalizadas) para los primeros 5 pares de relés:\")\n",
    "    for i in range(min(74, predicted_output.shape[1])):  # Mostrar solo los primeros 5 pares\n",
    "        print(f\"Par {i+1}:\")\n",
    "        print(f\"  Predicción: {predicted_output[0, i, :]}\")\n",
    "        print(f\"  Valor real: {true_output[i, :]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Asegúrate de tener pandas instalado (pip install pandas)\n",
    "import numpy as np\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "# --- 9. EXPORTACIÓN DETALLADA DE RESULTADOS (SOLO CONJUNTO DE ENTRENAMIENTO) ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- INICIANDO EXPORTACIÓN DETALLADA (SOLO DATOS DE ENTRENAMIENTO) ---\")\n",
    "\n",
    "# Verificar que las variables necesarias existen\n",
    "required_vars_export = ['final_model', 'scaler_input', 'scaler_target',\n",
    "                        'input_tensor', 'target_tensor', 'DEVICE', 'train_idx'] # Asegurar que train_idx exista\n",
    "\n",
    "if all(var in globals() for var in required_vars_export):\n",
    "    print(\"Variables necesarias encontradas. Procediendo con la exportación...\")\n",
    "\n",
    "    # Preparar listas para almacenar los datos aplanados\n",
    "    training_results_list = []\n",
    "\n",
    "    num_train_scenarios = len(train_idx)\n",
    "    if num_train_scenarios == 0:\n",
    "        print(\"Error: La lista de índices de entrenamiento (train_idx) está vacía.\")\n",
    "    else:\n",
    "        seq_len = input_tensor.shape[1]\n",
    "        num_input_features = input_tensor.shape[2]\n",
    "        num_output_features = target_tensor.shape[2]\n",
    "\n",
    "        # Definir nombres de columnas (GENÉRICOS - AJUSTA SEGÚN TUS DATOS)\n",
    "        input_cols = [f'Input_F{i+1}' for i in range(num_input_features)]\n",
    "        # ¡AJUSTA ESTOS NOMBRES A TDS, PICKUP, etc.!\n",
    "        target_cols = [f'Target_F{i+1}' for i in range(num_output_features)]\n",
    "        pred_cols = [f'Pred_F{i+1}' for i in range(num_output_features)] # Salidas del modelo desnormalizadas\n",
    "\n",
    "        print(f\"Se procesarán {num_train_scenarios} escenarios del conjunto de entrenamiento.\")\n",
    "        start_export_time = time.time()\n",
    "\n",
    "        # Iterar SOLAMENTE por los índices del conjunto de entrenamiento\n",
    "        count = 0\n",
    "        for scenario_index in train_idx:\n",
    "            count += 1\n",
    "            print(f\"Procesando escenario de entrenamiento {count}/{num_train_scenarios} (Índice Original: {scenario_index})...\")\n",
    "            try:\n",
    "                # Obtener datos originales del escenario actual\n",
    "                single_input_original = input_tensor[scenario_index]\n",
    "                single_target_original = target_tensor[scenario_index]\n",
    "\n",
    "                # Realizar la predicción desnormalizada para este escenario\n",
    "                # Esta función devuelve la salida del modelo ya desnormalizada\n",
    "                prediction_denormalized = predict_single_scenario(\n",
    "                    final_model,\n",
    "                    single_input_original,\n",
    "                    scaler_input,\n",
    "                    scaler_target,\n",
    "                    DEVICE\n",
    "                ) # Shape: (seq_len, output_features)\n",
    "\n",
    "                # Convertir target a numpy si es necesario\n",
    "                if isinstance(single_target_original, torch.Tensor):\n",
    "                    target_np_original = single_target_original.cpu().numpy()\n",
    "                else:\n",
    "                    target_np_original = np.array(single_target_original)\n",
    "\n",
    "                # Convertir input a numpy si es necesario\n",
    "                if isinstance(single_input_original, torch.Tensor):\n",
    "                     input_np_original = single_input_original.cpu().numpy()\n",
    "                else:\n",
    "                     input_np_original = np.array(single_input_original)\n",
    "\n",
    "                # Iterar por cada par de relés (paso de tiempo) dentro del escenario\n",
    "                for j in range(seq_len):\n",
    "                    row_data = {\n",
    "                        'Scenario_Index': scenario_index, # Índice Original\n",
    "                        'Relay_Pair_Index': j\n",
    "                    }\n",
    "                    # Añadir entradas originales\n",
    "                    for k in range(num_input_features):\n",
    "                        row_data[input_cols[k]] = input_np_original[j, k]\n",
    "                    # Añadir salidas objetivo originales (reales)\n",
    "                    for k in range(num_output_features):\n",
    "                        row_data[target_cols[k]] = target_np_original[j, k]\n",
    "                    # Añadir salidas predichas por el modelo (desnormalizadas)\n",
    "                    for k in range(num_output_features):\n",
    "                        row_data[pred_cols[k]] = prediction_denormalized[j, k]\n",
    "\n",
    "                    training_results_list.append(row_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n*** Error procesando el escenario de entrenamiento con índice original {scenario_index}: ***\")\n",
    "                print(e)\n",
    "                traceback.print_exc()\n",
    "                print(f\"*** Saltando escenario {scenario_index} ***\")\n",
    "                continue # Continuar con el siguiente escenario\n",
    "\n",
    "        # Crear DataFrame de Pandas con los resultados de entrenamiento\n",
    "        print(\"\\nCreando DataFrame con los resultados del conjunto de entrenamiento...\")\n",
    "        if training_results_list: # Solo si se procesó algo\n",
    "            try:\n",
    "                results_df_train = pd.DataFrame(training_results_list)\n",
    "                column_order = ['Scenario_Index', 'Relay_Pair_Index'] + input_cols + target_cols + pred_cols\n",
    "                results_df_train = results_df_train[column_order]\n",
    "\n",
    "                # Exportar a CSV\n",
    "                output_filename = 'training_set_predictions.csv'\n",
    "                print(f\"Exportando resultados a '{output_filename}'...\")\n",
    "                results_df_train.to_csv(output_filename, index=False, float_format='%.6f')\n",
    "\n",
    "                end_export_time = time.time()\n",
    "                print(\"\\n--- EXPORTACIÓN DE ENTRENAMIENTO COMPLETADA ---\")\n",
    "                print(f\"Resultados guardados en: {output_filename}\")\n",
    "                print(f\"Número total de filas exportadas (entrenamiento): {len(results_df_train)}\")\n",
    "                print(f\"Tiempo total de exportación: {end_export_time - start_export_time:.2f} segundos.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"\\n*** Error al crear o guardar el DataFrame/CSV de entrenamiento: ***\")\n",
    "                print(e)\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(\"\\nNo se procesaron datos de entrenamiento, no se generó archivo CSV.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo se puede ejecutar la exportación detallada para el conjunto de entrenamiento.\")\n",
    "    print(\"Asegúrate de que el entrenamiento se completó y las variables\")\n",
    "    print(f\"{required_vars_export} están disponibles en el entorno.\")\n",
    "    print(\"Verifica también que la función 'predict_single_scenario' esté definida.\")\n",
    "\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
